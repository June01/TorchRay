

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Attribution &mdash; TorchRay beta documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/equations.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Benchmarking" href="benchmark.html" />
    <link rel="prev" title="Welcome to TorchRay" href="index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> TorchRay
          

          
          </a>

          
            
            
              <div class="version">
                1.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Attribution</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#backpropagation-methods">Backpropagation methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#changing-the-backward-propagation-rules">Changing the backward propagation rules</a></li>
<li class="toctree-l3"><a class="reference internal" href="#probing-intermediate-activations-and-gradients">Probing intermediate activations and gradients</a></li>
<li class="toctree-l3"><a class="reference internal" href="#limitations">Limitations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#perturbation-methods">Perturbation methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-torchray.attribution.deconvnet">DeConvNet</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#theory">Theory</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#module-torchray.attribution.excitation_backprop">Excitation backprop</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#contrastive-variant">Contrastive variant</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id6">Theory</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#module-torchray.attribution.extremal_perturbation">Extremal perturbation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#perturbation-types">Perturbation types</a></li>
<li class="toctree-l3"><a class="reference internal" href="#extremal-perturbation-variants">Extremal perturbation variants</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#module-torchray.attribution.gradient">Gradient</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-torchray.attribution.grad_cam">Grad-CAM</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id11">Theory</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#module-torchray.attribution.guided_backprop">Guided backprop</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id13">Theory</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#module-torchray.attribution.linear_approx">Linear approximation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-torchray.attribution.rise">RISE</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-torchray.attribution.common">Common code</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">Utilities</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">TorchRay</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Attribution</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/attribution.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="attribution">
<span id="backprop"></span><h1>Attribution<a class="headerlink" href="#attribution" title="Permalink to this headline">¶</a></h1>
<p><em>Attribution</em> is the problem of determining which part of the input,
e.g. an image, is responsible for the value computed by a predictor
such as a neural network.</p>
<p>Formally, let <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> be the input to a convolutional neural
network, e.g., a <span class="math notranslate nohighlight">\(N \times C \times H \times W\)</span> real tensor. The neural
network is a function <span class="math notranslate nohighlight">\(\Phi\)</span> mapping <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> to a scalar
output <span class="math notranslate nohighlight">\(z \in \mathbb{R}\)</span>. Thus the goal is to find which of the
elements of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> are “most responsible” for the outcome
<span class="math notranslate nohighlight">\(z\)</span>.</p>
<p>Some attribution methods are “black box” approaches, in the sense that they
ignore the nature of the function <span class="math notranslate nohighlight">\(\Phi\)</span> (however, most assume that it is
at least possible to compute the gradient of <span class="math notranslate nohighlight">\(\Phi\)</span> efficiently). Most
attribution methods, however, are “white box” approaches, in the sense that
they exploit the knowledge of the structure of <span class="math notranslate nohighlight">\(\Phi\)</span>.</p>
<p><a class="reference internal" href="#backpropagation"><span class="std std-ref">Backpropagation methods</span></a> are “white box” visualization
approaches that build on backpropagation, thus leveraging the functionality
already implemented in standard deep learning packages toolboxes such as
PyTorch.</p>
<p><a class="reference internal" href="#perturbation"><span class="std std-ref">Perturbation methods</span></a> are “black box” visualization
approaches that generate attribution visualizations by perturbing the input
and observing the changes in a model’s output.</p>
<p>TorchRay implements the following methods:</p>
<ul class="simple">
<li><p>Backpropagation methods</p>
<ul>
<li><p>Deconvolution (<a class="reference internal" href="#module-torchray.attribution.deconvnet" title="torchray.attribution.deconvnet"><code class="xref py py-mod docutils literal notranslate"><span class="pre">deconvnet</span></code></a>)</p></li>
<li><p>Excitation backpropagation (<a class="reference internal" href="#module-torchray.attribution.excitation_backprop" title="torchray.attribution.excitation_backprop"><code class="xref py py-mod docutils literal notranslate"><span class="pre">excitation_backprop</span></code></a>)</p></li>
<li><p>Gradient <a class="footnote-reference brackets" href="#id3" id="id1">1</a> (<a class="reference internal" href="#module-torchray.attribution.gradient" title="torchray.attribution.gradient"><code class="xref py py-mod docutils literal notranslate"><span class="pre">gradient</span></code></a>)</p></li>
<li><p>Grad-CAM (<a class="reference internal" href="#module-torchray.attribution.grad_cam" title="torchray.attribution.grad_cam"><code class="xref py py-mod docutils literal notranslate"><span class="pre">grad_cam</span></code></a>)</p></li>
<li><p>Guided backpropagation (<a class="reference internal" href="#module-torchray.attribution.guided_backprop" title="torchray.attribution.guided_backprop"><code class="xref py py-mod docutils literal notranslate"><span class="pre">guided_backprop</span></code></a>)</p></li>
<li><p>Linear approximation (<a class="reference internal" href="#module-torchray.attribution.linear_approx" title="torchray.attribution.linear_approx"><code class="xref py py-mod docutils literal notranslate"><span class="pre">linear_approx</span></code></a>)</p></li>
</ul>
</li>
<li><p>Perturbation methods</p>
<ul>
<li><p>Extremal perturbation <a class="footnote-reference brackets" href="#id3" id="id2">1</a> (<a class="reference internal" href="#module-torchray.attribution.extremal_perturbation" title="torchray.attribution.extremal_perturbation"><code class="xref py py-mod docutils literal notranslate"><span class="pre">extremal_perturbation</span></code></a>)</p></li>
<li><p>RISE (<a class="reference internal" href="#module-torchray.attribution.rise" title="torchray.attribution.rise"><code class="xref py py-mod docutils literal notranslate"><span class="pre">rise</span></code></a>)</p></li>
</ul>
</li>
</ul>
<p class="rubric">Footnotes</p>
<dl class="footnote brackets">
<dt class="label" id="id3"><span class="brackets">1</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id2">2</a>)</span></dt>
<dd><p>The <a class="reference internal" href="#module-torchray.attribution.gradient" title="torchray.attribution.gradient"><code class="xref py py-mod docutils literal notranslate"><span class="pre">gradient</span></code></a> and <a class="reference internal" href="#module-torchray.attribution.extremal_perturbation" title="torchray.attribution.extremal_perturbation"><code class="xref py py-mod docutils literal notranslate"><span class="pre">extremal_perturbation</span></code></a> methods actually
straddle the boundaries between white and black box methods, as they
only require the ability to compute the gradient of the predictor,
which does not necessarily require to know the predictor internals.
However, in TorchRay both are implemented using backpropagation.</p>
</dd>
</dl>
<div class="section" id="backpropagation-methods">
<span id="backpropagation"></span><h2>Backpropagation methods<a class="headerlink" href="#backpropagation-methods" title="Permalink to this headline">¶</a></h2>
<p>Backpropagation methods work by tweaking the backpropagation algorithm that, on
its own, computes the gradient of tensor functions. Formally, a neural network
<span class="math notranslate nohighlight">\(\Phi\)</span> is a collection <span class="math notranslate nohighlight">\(\Phi_1,\dots,\Phi_n\)</span> of <span class="math notranslate nohighlight">\(n\)</span> layers.
Each layer is in itself a “smaller” function inputting and outputting tensors,
called <em>activations</em> (for simplicity, we call activations the network input and
parameter tensors as well). Layers are interconnected in a <em>Directed Acyclic
Graph</em> (DAG). The DAG is bipartite with some nodes representing the activation
tensors and the other nodes representing the layers, with interconnections
between layers and input/output tensors in the obvious way. The DAG sources are
the network’s input and parameter tensors and the DAG sinks are the network’s
output tensors.</p>
<p>The main goal of a deep neural network toolbox such as PyTorch is to evaluate
the function <span class="math notranslate nohighlight">\(\Phi\)</span> implemented by the DAG as well as its gradients with
respect to various tensors (usually the model parameters). The calculation of
the gradients, which uses backpropagation, associates to the forward DAG a
backward DAG, obtained as follows:</p>
<ul class="simple">
<li><p>Activation tensors <span class="math notranslate nohighlight">\(\mathbf{x}_j\)</span> become gradient tensors
<span class="math notranslate nohighlight">\(d\mathbf{x}_j\)</span> (preserving their shape).</p></li>
<li><p>Forward layers <span class="math notranslate nohighlight">\(\Phi_i\)</span> become backward layers <span class="math notranslate nohighlight">\(\Phi_i^*\)</span>.</p></li>
<li><p>All arrows are reversed.</p></li>
<li><p>Additional arrows connecting the activation tensors <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span>
as inputs to the corresponding backward function <span class="math notranslate nohighlight">\(\Phi_i^*\)</span> are added
as well.</p></li>
</ul>
<p>Backpropagation methods modify the backward graph in order to generate a
visualization of the network forward pass. Additionally, inputs as well as
intermediate activations can be inspected to obtain different visualizations.
These two concepts are explained next.</p>
<div class="section" id="changing-the-backward-propagation-rules">
<h3>Changing the backward propagation rules<a class="headerlink" href="#changing-the-backward-propagation-rules" title="Permalink to this headline">¶</a></h3>
<p>Changing the backward propagation rules amounts to redefining the functions
<span class="math notranslate nohighlight">\(\Phi_i^*\)</span>. After doing so, the “gradients” computed by backpropagation
change their meaning into something useful for visualization. We call these
modified gradients <em>pseudo-gradients</em>.</p>
<p>TorchRay provides a number of context managers that enable patching PyTorch
functions on the fly in order to change the backward propagation rules for
a segment of code. For example, let <code class="docutils literal notranslate"><span class="pre">x</span></code> be an input tensor and <code class="docutils literal notranslate"><span class="pre">model</span></code>
a deep classification network.  Furthermore, let <code class="docutils literal notranslate"><span class="pre">category_id</span></code> be the
index of the class for which we want to attribute input regions. The following
code uses <a class="reference internal" href="#module-torchray.attribution.guided_backprop" title="torchray.attribution.guided_backprop"><code class="xref py py-mod docutils literal notranslate"><span class="pre">guided_backprop</span></code></a> to compute and store the pseudo gradient in
<code class="docutils literal notranslate"><span class="pre">x.grad</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchray.attribution.guided_backprop</span> <span class="kn">import</span> <span class="n">GuidedBackpropContext</span>

<span class="n">x</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>

<span class="k">with</span> <span class="n">GuidedBackpropContext</span><span class="p">():</span>
      <span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
      <span class="n">z</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">category_id</span><span class="p">]</span>
      <span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<p>At this point, <code class="docutils literal notranslate"><span class="pre">x.grad</span></code> contains the “guided gradient” computed by this
method. This gradient is usually flattened along the channel dimension to
produce a saliency map for visualization:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchray.attribution.common</span> <span class="kn">import</span> <span class="n">gradient_to_saliency</span>

<span class="n">saliency</span> <span class="o">=</span> <span class="n">gradient_to_saliency</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>TorchRay contains also some wrapper code, such as
<a class="reference internal" href="#torchray.attribution.guided_backprop.guided_backprop" title="torchray.attribution.guided_backprop.guided_backprop"><code class="xref py py-func docutils literal notranslate"><span class="pre">guided_backprop.guided_backprop()</span></code></a>, that combine these steps in a way
that would work for common networks.</p>
</div>
<div class="section" id="probing-intermediate-activations-and-gradients">
<h3>Probing intermediate activations and gradients<a class="headerlink" href="#probing-intermediate-activations-and-gradients" title="Permalink to this headline">¶</a></h3>
<p>Most visualization methods are based on inspecting the activations when the
network is evaluated and the pseudo-gradients during backpropagation. This is
generally easy for input tensors. For intermediate tensors, when using PyTorch
functional interface, this is also easy: simply use <code class="docutils literal notranslate"><span class="pre">retain_grad_(True)</span></code> in
order to retain the gradient of an intermediate tensor:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.nn.functional</span> <span class="kn">import</span> <span class="n">relu</span><span class="p">,</span> <span class="n">conv2d</span>
<span class="kn">from</span> <span class="nn">torchray.attribution</span> <span class="kn">import</span> <span class="n">GuidedBackpropContext</span>

<span class="k">with</span> <span class="n">GuidedBackpropContext</span><span class="p">():</span>
      <span class="n">y</span> <span class="o">=</span> <span class="n">conv2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
      <span class="n">y</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
      <span class="n">y</span><span class="o">.</span><span class="n">retain_grad_</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
      <span class="n">z</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">y</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="n">class_index</span><span class="p">]</span>
      <span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="c1"># Now y and y.grad contain the activation and guided gradient,</span>
<span class="c1"># respectively.</span>
</pre></div>
</div>
<p>However, in PyTorch most network components are implemented as
<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> objects. In this case, is not obvious how to access a
specific layer’s information. In order to simplify this process, the library
provides the <code class="xref py py-class docutils literal notranslate"><span class="pre">Probe</span></code> class:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.nn.functional</span> <span class="kn">import</span> <span class="n">relu</span><span class="p">,</span> <span class="n">conv2d</span>
<span class="kn">from</span> <span class="nn">torchray.attribution.guided_backprop</span> <span class="kn">import</span> <span class="n">GuidedBackpropContext</span>
<span class="kn">import</span> <span class="nn">torchray.attribution.Probe</span>

<span class="c1"># Attach a probe to the last conv layer.</span>
<span class="n">probe</span> <span class="o">=</span> <span class="n">Probe</span><span class="p">(</span><span class="n">alexnet</span><span class="o">.</span><span class="n">features</span><span class="p">[</span><span class="mi">11</span><span class="p">])</span>

<span class="k">with</span> <span class="n">GuidedBackpropContext</span><span class="p">():</span>
      <span class="n">y</span> <span class="o">=</span> <span class="n">alexnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
      <span class="n">z</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">class_index</span><span class="p">]</span>
      <span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="c1"># Now probe.data[0] and probe.data[0].grad contain</span>
<span class="c1"># the activations and guided gradients.</span>
</pre></div>
</div>
<p>The probe automatically applies <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.requires_grad_()</span></code> and
<code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.retain_grad_()</span></code> as needed. You can use <code class="docutils literal notranslate"><span class="pre">probe.remove()</span></code> to
remove the probe from the network once you are done.</p>
</div>
<div class="section" id="limitations">
<h3>Limitations<a class="headerlink" href="#limitations" title="Permalink to this headline">¶</a></h3>
<p>Except for the gradient method, backpropagation methods require modifying
the backward function of each layer. TorchRay implements the rules
necessary to do so as originally defined by each authors’ method.
However, as new neural network layers are introduced, it is possible
that the default behavior, which is to not change backpropagation, may
be inappropriate or suboptimal for them.</p>
</div>
</div>
<div class="section" id="perturbation-methods">
<span id="perturbation"></span><h2>Perturbation methods<a class="headerlink" href="#perturbation-methods" title="Permalink to this headline">¶</a></h2>
<p>Perturbation methods work by changing the input to the neural network in a
controlled manner, observing the outcome on the output generated by the
network. Attribution can be achieved by occluding (setting to zero) specific
parts of the image and checking whether this has a strong effect on the output.
This can be thought of as a form of sensitivity analysis which is still
specific to a given input, but is not differential as for the gradient method.</p>
</div>
<div class="section" id="module-torchray.attribution.deconvnet">
<span id="deconvnet"></span><h2>DeConvNet<a class="headerlink" href="#module-torchray.attribution.deconvnet" title="Permalink to this headline">¶</a></h2>
<p>This module implements the <em>deconvolution</em>  method of <a class="reference internal" href="#deconv" id="id4"><span>[DECONV]</span></a> for visualizing
deep networks. The simplest interface is given by the <a class="reference internal" href="#torchray.attribution.deconvnet.deconvnet" title="torchray.attribution.deconvnet.deconvnet"><code class="xref py py-func docutils literal notranslate"><span class="pre">deconvnet()</span></code></a>
function:</p>
<div class="highlight-python notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchray.attribution.deconvnet</span> <span class="kn">import</span> <span class="n">deconvnet</span>
<span class="kn">from</span> <span class="nn">torchray.benchmark</span> <span class="kn">import</span> <span class="n">get_example_data</span><span class="p">,</span> <span class="n">plot_example</span>

<span class="c1"># Obtain example data.</span>
<span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">category_id</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">get_example_data</span><span class="p">()</span>

<span class="c1"># DeConvNet method.</span>
<span class="n">saliency</span> <span class="o">=</span> <span class="n">deconvnet</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">category_id</span><span class="p">)</span>

<span class="c1"># Plots.</span>
<span class="n">plot_example</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">saliency</span><span class="p">,</span> <span class="s1">&#39;deconvnet&#39;</span><span class="p">,</span> <span class="n">category_id</span><span class="p">)</span>
</pre></div>
</td></tr></table></div>
<p>Alternatively, it is possible to run the method “manually”. DeConvNet is a
backpropagation method, and thus works by changing the definition of the
backward functions of some layers. The modified ReLU is implemented by class
<code class="xref py py-class docutils literal notranslate"><span class="pre">DeConvNetReLU</span></code>; however, this is rarely used directly; instead, one
uses the <a class="reference internal" href="#torchray.attribution.deconvnet.DeConvNetContext" title="torchray.attribution.deconvnet.DeConvNetContext"><code class="xref py py-class docutils literal notranslate"><span class="pre">DeConvNetContext</span></code></a> context instead, as follows:</p>
<div class="highlight-python notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchray.attribution.common</span> <span class="kn">import</span> <span class="n">gradient_to_saliency</span>
<span class="kn">from</span> <span class="nn">torchray.attribution.deconvnet</span> <span class="kn">import</span> <span class="n">DeConvNetContext</span>
<span class="kn">from</span> <span class="nn">torchray.benchmark</span> <span class="kn">import</span> <span class="n">get_example_data</span><span class="p">,</span> <span class="n">plot_example</span>

<span class="c1"># Obtain example data.</span>
<span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">category_id</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">get_example_data</span><span class="p">()</span>

<span class="c1"># DeConvNet method.</span>
<span class="n">x</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>

<span class="k">with</span> <span class="n">DeConvNetContext</span><span class="p">():</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">category_id</span><span class="p">]</span>
    <span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="n">saliency</span> <span class="o">=</span> <span class="n">gradient_to_saliency</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Plots.</span>
<span class="n">plot_example</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">saliency</span><span class="p">,</span> <span class="s1">&#39;deconvnet&#39;</span><span class="p">,</span> <span class="n">category_id</span><span class="p">)</span>
</pre></div>
</td></tr></table></div>
<p>See also <a class="reference internal" href="#backpropagation"><span class="std std-ref">Backprogation methods</span></a> for further examples
and discussion.</p>
<div class="section" id="theory">
<h3>Theory<a class="headerlink" href="#theory" title="Permalink to this headline">¶</a></h3>
<p>The only change is a modified definition of the backward ReLU function:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\operatorname{ReLU}^*(x,p) =
\begin{cases}
p, &amp; \mathrm{if}~ p &gt; 0,\\
0, &amp; \mathrm{otherwise} \\
\end{cases}\end{split}\]</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>DeConvNets are defined for “standard” networks that use ReLU operations.
Further modifications may be required for more complex or new networks
that use other type of non-linearities.</p>
</div>
<p class="rubric">References</p>
<dl class="citation">
<dt class="label" id="deconv"><span class="brackets"><a class="fn-backref" href="#id4">DECONV</a></span></dt>
<dd><p>Zeiler and Fergus,
<em>Visualizing and Understanding Convolutional Networks</em>,
ECCV 2014,
<a class="reference external" href="https://doi.org/10.1007/978-3-319-10590-1_53">https://doi.org/10.1007/978-3-319-10590-1_53</a>.</p>
</dd>
</dl>
<dl class="class">
<dt id="torchray.attribution.deconvnet.DeConvNetContext">
<em class="property">class </em><code class="sig-prename descclassname">torchray.attribution.deconvnet.</code><code class="sig-name descname">DeConvNetContext</code><a class="reference internal" href="_modules/torchray/attribution/deconvnet.html#DeConvNetContext"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchray.attribution.deconvnet.DeConvNetContext" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchray.attribution.common.ReLUContext" title="torchray.attribution.common.ReLUContext"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchray.attribution.common.ReLUContext</span></code></a></p>
<p>DeConvNet context.</p>
<p>This context modifies the computation of gradient to match the DeConvNet
definition.</p>
<p>See <a class="reference internal" href="#module-torchray.attribution.deconvnet" title="torchray.attribution.deconvnet"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torchray.attribution.deconvnet</span></code></a> for how to use it.</p>
</dd></dl>

<dl class="function">
<dt id="torchray.attribution.deconvnet.deconvnet">
<code class="sig-prename descclassname">torchray.attribution.deconvnet.</code><code class="sig-name descname">deconvnet</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">context_builder=&lt;class 'torchray.attribution.deconvnet.DeConvNetContext'&gt;</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchray/attribution/deconvnet.html#deconvnet"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchray.attribution.deconvnet.deconvnet" title="Permalink to this definition">¶</a></dt>
<dd><p>DeConvNet method.</p>
<p>The function takes the same arguments as <a class="reference internal" href="#torchray.attribution.common.saliency" title="torchray.attribution.common.saliency"><code class="xref py py-func docutils literal notranslate"><span class="pre">common.saliency()</span></code></a>, with
the defaults required to apply the DeConvNet method, and supports the
same arguments and return values.</p>
</dd></dl>

</div>
</div>
<div class="section" id="module-torchray.attribution.excitation_backprop">
<span id="excitation-backprop"></span><h2>Excitation backprop<a class="headerlink" href="#module-torchray.attribution.excitation_backprop" title="Permalink to this headline">¶</a></h2>
<p>This module provides an implementation of the <em>excitation backpropagation</em>
method of <a class="reference internal" href="#ebp" id="id5"><span>[EBP]</span></a>  for saliency visualization. It is a backpropagation method,
and thus works by changing the definition of the backward functions of some
layers.</p>
<p>In simple cases, the <a class="reference internal" href="#torchray.attribution.excitation_backprop.excitation_backprop" title="torchray.attribution.excitation_backprop.excitation_backprop"><code class="xref py py-func docutils literal notranslate"><span class="pre">excitation_backprop()</span></code></a> function can be used to obtain
the required visualization, as in the following example:</p>
<div class="highlight-python notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchray.attribution.excitation_backprop</span> <span class="kn">import</span> <span class="n">excitation_backprop</span>
<span class="kn">from</span> <span class="nn">torchray.benchmark</span> <span class="kn">import</span> <span class="n">get_example_data</span><span class="p">,</span> <span class="n">plot_example</span>

<span class="c1"># Obtain example data.</span>
<span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">category_id</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">get_example_data</span><span class="p">()</span>

<span class="c1"># Contrastive excitation backprop.</span>
<span class="n">saliency</span> <span class="o">=</span> <span class="n">excitation_backprop</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">x</span><span class="p">,</span>
    <span class="n">category_id</span><span class="p">,</span>
    <span class="n">saliency_layer</span><span class="o">=</span><span class="s1">&#39;features.9&#39;</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Plots.</span>
<span class="n">plot_example</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">saliency</span><span class="p">,</span> <span class="s1">&#39;excitation backprop&#39;</span><span class="p">,</span> <span class="n">category_id</span><span class="p">)</span>
</pre></div>
</td></tr></table></div>
<p>Alternatively, you can explicitly use the <a class="reference internal" href="#torchray.attribution.excitation_backprop.ExcitationBackpropContext" title="torchray.attribution.excitation_backprop.ExcitationBackpropContext"><code class="xref py py-class docutils literal notranslate"><span class="pre">ExcitationBackpropContext</span></code></a>,
as follows:</p>
<div class="highlight-python notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchray.attribution.common</span> <span class="kn">import</span> <span class="n">Probe</span><span class="p">,</span> <span class="n">get_module</span>
<span class="kn">from</span> <span class="nn">torchray.attribution.excitation_backprop</span> <span class="kn">import</span> <span class="n">ExcitationBackpropContext</span>
<span class="kn">from</span> <span class="nn">torchray.attribution.excitation_backprop</span> <span class="kn">import</span> <span class="n">gradient_to_excitation_backprop_saliency</span>
<span class="kn">from</span> <span class="nn">torchray.benchmark</span> <span class="kn">import</span> <span class="n">get_example_data</span><span class="p">,</span> <span class="n">plot_example</span>

<span class="c1"># Obtain example data.</span>
<span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">category_id</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">get_example_data</span><span class="p">()</span>

<span class="c1"># Contrastive excitation backprop.</span>
<span class="n">saliency_layer</span> <span class="o">=</span> <span class="n">get_module</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;features.9&#39;</span><span class="p">)</span>
<span class="n">saliency_probe</span> <span class="o">=</span> <span class="n">Probe</span><span class="p">(</span><span class="n">saliency_layer</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="s1">&#39;output&#39;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">ExcitationBackpropContext</span><span class="p">():</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">category_id</span><span class="p">]</span>
    <span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="n">saliency</span> <span class="o">=</span> <span class="n">gradient_to_excitation_backprop_saliency</span><span class="p">(</span><span class="n">saliency_probe</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">saliency_probe</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>

<span class="c1"># Plots.</span>
<span class="n">plot_example</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">saliency</span><span class="p">,</span> <span class="s1">&#39;excitation backprop&#39;</span><span class="p">,</span> <span class="n">category_id</span><span class="p">)</span>
</pre></div>
</td></tr></table></div>
<p>See also <a class="reference internal" href="#backprop"><span class="std std-ref">Attribution</span></a> for further examples and discussion.</p>
<div class="section" id="contrastive-variant">
<h3>Contrastive variant<a class="headerlink" href="#contrastive-variant" title="Permalink to this headline">¶</a></h3>
<p>The contrastive variant of excitation backprop passes the data twice through
the network. The first pass is used to obtain “contrast” activations at some
intermediate layer <code class="docutils literal notranslate"><span class="pre">contrast_layer</span></code>. The latter is obtained by flipping the
sign of the last classification layer <code class="docutils literal notranslate"><span class="pre">classifier_layer</span></code>. The visualization
is then obtained at some earlier <code class="docutils literal notranslate"><span class="pre">input_layer</span></code>. The function
<a class="reference internal" href="#torchray.attribution.excitation_backprop.contrastive_excitation_backprop" title="torchray.attribution.excitation_backprop.contrastive_excitation_backprop"><code class="xref py py-func docutils literal notranslate"><span class="pre">contrastive_excitation_backprop()</span></code></a> can be used to compute this saliency:</p>
<div class="highlight-python notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchray.attribution.excitation_backprop</span> <span class="kn">import</span> <span class="n">contrastive_excitation_backprop</span>
<span class="kn">from</span> <span class="nn">torchray.benchmark</span> <span class="kn">import</span> <span class="n">get_example_data</span><span class="p">,</span> <span class="n">plot_example</span>

<span class="c1"># Obtain example data.</span>
<span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">category_id</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">get_example_data</span><span class="p">()</span>

<span class="c1"># Contrastive excitation backprop.</span>
<span class="n">saliency</span> <span class="o">=</span> <span class="n">contrastive_excitation_backprop</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">x</span><span class="p">,</span>
    <span class="n">category_id</span><span class="p">,</span>
    <span class="n">saliency_layer</span><span class="o">=</span><span class="s1">&#39;features.9&#39;</span><span class="p">,</span>
    <span class="n">contrast_layer</span><span class="o">=</span><span class="s1">&#39;features.30&#39;</span><span class="p">,</span>
    <span class="n">classifier_layer</span><span class="o">=</span><span class="s1">&#39;classifier.6&#39;</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Plots.</span>
<span class="n">plot_example</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">saliency</span><span class="p">,</span> <span class="s1">&#39;contrastive excitation backprop&#39;</span><span class="p">,</span> <span class="n">category_id</span><span class="p">)</span>
</pre></div>
</td></tr></table></div>
<p>This can also be done “manually”, as follows:</p>
<div class="highlight-python notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchray.attribution.common</span> <span class="kn">import</span> <span class="n">Probe</span><span class="p">,</span> <span class="n">get_module</span>
<span class="kn">from</span> <span class="nn">torchray.attribution.excitation_backprop</span> <span class="kn">import</span> <span class="n">ExcitationBackpropContext</span>
<span class="kn">from</span> <span class="nn">torchray.attribution.excitation_backprop</span> <span class="kn">import</span> <span class="n">gradient_to_contrastive_excitation_backprop_saliency</span>
<span class="kn">from</span> <span class="nn">torchray.benchmark</span> <span class="kn">import</span> <span class="n">get_example_data</span><span class="p">,</span> <span class="n">plot_example</span>

<span class="c1"># Obtain example data.</span>
<span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">category_id</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">get_example_data</span><span class="p">()</span>

<span class="c1"># Contrastive excitation backprop.</span>
<span class="n">input_layer</span> <span class="o">=</span> <span class="n">get_module</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;features.9&#39;</span><span class="p">)</span>
<span class="n">contrast_layer</span> <span class="o">=</span> <span class="n">get_module</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;features.30&#39;</span><span class="p">)</span>
<span class="n">classifier_layer</span> <span class="o">=</span> <span class="n">get_module</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;classifier.6&#39;</span><span class="p">)</span>

<span class="n">input_probe</span> <span class="o">=</span> <span class="n">Probe</span><span class="p">(</span><span class="n">input_layer</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="s1">&#39;output&#39;</span><span class="p">)</span>
<span class="n">contrast_probe</span> <span class="o">=</span> <span class="n">Probe</span><span class="p">(</span><span class="n">contrast_layer</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="s1">&#39;output&#39;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">ExcitationBackpropContext</span><span class="p">():</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">category_id</span><span class="p">]</span>
    <span class="n">classifier_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">neg_</span><span class="p">()</span>
    <span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="n">classifier_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">neg_</span><span class="p">()</span>

    <span class="n">contrast_probe</span><span class="o">.</span><span class="n">contrast</span> <span class="o">=</span> <span class="p">[</span><span class="n">contrast_probe</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grad</span><span class="p">]</span>

    <span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">category_id</span><span class="p">]</span>
    <span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="n">saliency</span> <span class="o">=</span> <span class="n">gradient_to_contrastive_excitation_backprop_saliency</span><span class="p">(</span><span class="n">input_probe</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">input_probe</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>
<span class="n">contrast_probe</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>

<span class="c1"># Plots.</span>
<span class="n">plot_example</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">saliency</span><span class="p">,</span> <span class="s1">&#39;contrastive excitation backprop&#39;</span><span class="p">,</span> <span class="n">category_id</span><span class="p">)</span>
</pre></div>
</td></tr></table></div>
</div>
<div class="section" id="id6">
<h3>Theory<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p>Excitation backprop modifies the backward version of all linear layers in the
network. For a simple 1D case, let the forward layer be given by:</p>
<div class="math notranslate nohighlight">
\[y_i = \sum_{j=1}^N w_{ij} x_j\]</div>
<p>where <span class="math notranslate nohighlight">\(x \in \mathbb{R}^B\)</span> is the input and
<span class="math notranslate nohighlight">\(w \in\mathbb{R}^{M \times N}\)</span> the weight matrix. On the way back, if
<span class="math notranslate nohighlight">\(p \in \mathbb{R}^{N}\)</span> is the output pseudo-gradient, the input
pseudo-gradient <span class="math notranslate nohighlight">\(p'\)</span> is given by:</p>
<div class="math notranslate nohighlight" id="equation-linear-back">
<span class="eqno">(1)<a class="headerlink" href="#equation-linear-back" title="Permalink to this equation">¶</a></span>\[p'_j =
\sum_{i=1}^N
    \frac
    {w^+_{ij} x_j}
    {\sum_{k=1}^ Nw^+_{ik} x_k} p_i
\quad\text{where}\quad
w^+_{ij} = \max\{0, w_{ij}\}\]</div>
<p>Note that <a class="reference internal" href="#ebp" id="id7"><span>[EBP]</span></a> assumes that the input activations <span class="math notranslate nohighlight">\(x\)</span> are always
non-negative. This is often true, as linear layer are preceded by non-negative
activation functions such as ReLU, so there is nothing to be done.</p>
<p>Note also that we can rearrange <a class="reference internal" href="#equation-linear-back">(1)</a> as</p>
<div class="math notranslate nohighlight">
\[p'_j =
x_j
\sum_{i=1}^N
    w^+_{ij}
    \hat p_i,
\quad\text{where}\quad
\hat p_i =
    \frac
    {p_i}
    {\sum_{k=1}^ Nw^+_{ik} x_k}.\]</div>
<p>Here <span class="math notranslate nohighlight">\(\hat p\)</span> is a normalized version of the output pseudo-gradient and
the summation is the operation performed by the standard backward function for
the linear layer given as input <span class="math notranslate nohighlight">\(\hat p\)</span>.</p>
<p>All linear layers, including convolution and deconvolution layers as well as
average pooling layers, can be processed in this manner. In general, let
<span class="math notranslate nohighlight">\(y = f(x,w)\)</span> be a linear layer. In order to compute <a class="reference internal" href="#equation-linear-back">(1)</a>, we
can expand it as:</p>
<div class="math notranslate nohighlight">
\[p' = x \odot f^*(x, w^+, p \oslash f(x, w^+))\]</div>
<p>where <span class="math notranslate nohighlight">\(f^*(x, w, p)\)</span> is the standard backward function (vector-Jacobian
product) for the linear layer <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(\odot\)</span> and <span class="math notranslate nohighlight">\(\oslash\)</span>
denote element-wise multiplication and division, respectively.</p>
<p>The <strong>contrastive variant</strong> of excitation backprop is similar, but uses the
idea of contrasting the excitation for one class with the ones of all the
others.</p>
<p>In order to obtain the “contrast” signal, excitation backprop is run as before
except for the last <em>classification</em> linear layer. In order to backpropagate
the excitation for this layer only, the weight parameter <span class="math notranslate nohighlight">\(w\)</span> is replaced
with its opposite <span class="math notranslate nohighlight">\(-w\)</span>. Then, the excitations are backpropagated to an
intermediate <em>contrast</em> linear layer as normal.</p>
<p>Once the contrast activations have been obtained, excitation backprop is run
again, this time with the “default” weights even for the last linear layer.
However, during backpropagation, when the contrast layer is reached again, the
contrast is subtracted from the excitations. Then, the excitations are
propagated backward as usual.</p>
<p class="rubric">References</p>
<dl class="citation">
<dt class="label" id="ebp"><span class="brackets">EBP</span><span class="fn-backref">(<a href="#id5">1</a>,<a href="#id7">2</a>)</span></dt>
<dd><p>Jianming Zhang, Zhe Lin, Jonathan Brandt, Xiaohui Shen, Stan
Sclaroff, <em>Top-down Neural Attention by Excitation Backprop</em>,
ECCV 2016, <a class="reference external" href="https://arxiv.org/abs/1608.00507">https://arxiv.org/abs/1608.00507</a>.</p>
</dd>
</dl>
<dl class="function">
<dt id="torchray.attribution.excitation_backprop.contrastive_excitation_backprop">
<code class="sig-prename descclassname">torchray.attribution.excitation_backprop.</code><code class="sig-name descname">contrastive_excitation_backprop</code><span class="sig-paren">(</span><em class="sig-param">model</em>, <em class="sig-param">input</em>, <em class="sig-param">target</em>, <em class="sig-param">saliency_layer</em>, <em class="sig-param">contrast_layer</em>, <em class="sig-param">classifier_layer=None</em>, <em class="sig-param">resize=False</em>, <em class="sig-param">resize_mode='bilinear'</em>, <em class="sig-param">get_backward_gradient=&lt;function get_backward_gradient&gt;</em>, <em class="sig-param">debug=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchray/attribution/excitation_backprop.html#contrastive_excitation_backprop"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchray.attribution.excitation_backprop.contrastive_excitation_backprop" title="Permalink to this definition">¶</a></dt>
<dd><p>Contrastive excitation backprop.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>) – a model.</p></li>
<li><p><strong>input</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code>) – input tensor.</p></li>
<li><p><strong>target</strong> (int or <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code>) – target label(s).</p></li>
<li><p><strong>saliency_layer</strong> (str or <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>) – name of the saliency
layer (str) or the layer itself (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>) in
the model at which to visualize.</p></li>
<li><p><strong>contrast_layer</strong> (str or <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>) – name of the contrast
layer (str) or the layer itself (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>).</p></li>
<li><p><strong>classifier_layer</strong> (str or <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>, optional) – name of
the last classifier layer (str) or the layer itself
(<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>). Defaults to <code class="docutils literal notranslate"><span class="pre">None</span></code>, in which case
the functions tries to automatically identify the last layer.
Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>resize</strong> (<em>bool</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – If True resizes the saliency map to
the same size as <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>. It is also possible to pass a
(width, height) tuple to specify an arbitrary size. Default:
<code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>resize_mode</strong> (<em>str</em><em>, </em><em>optional</em>) – Specify the resampling mode.
Default: <code class="docutils literal notranslate"><span class="pre">'bilinear'</span></code>.</p></li>
<li><p><strong>get_backward_gradient</strong> (<em>function</em><em>, </em><em>optional</em>) – function that generates
gradient tensor to backpropagate. Default:
<a class="reference internal" href="#torchray.attribution.common.get_backward_gradient" title="torchray.attribution.common.get_backward_gradient"><code class="xref py py-func docutils literal notranslate"><span class="pre">common.get_backward_gradient()</span></code></a>.</p></li>
<li><p><strong>debug</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, also return
<code class="xref py py-class docutils literal notranslate"><span class="pre">collections.OrderedDict</span></code> of <a class="reference internal" href="#torchray.attribution.common.Probe" title="torchray.attribution.common.Probe"><code class="xref py py-class docutils literal notranslate"><span class="pre">common.Probe</span></code></a> objects
attached to all named modules in the model. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">debug</span></code> is False, returns a
<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code> saliency map at <code class="xref py py-attr docutils literal notranslate"><span class="pre">saliency_layer</span></code>.
Otherwise, returns a tuple of a <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code> saliency map
at <code class="xref py py-attr docutils literal notranslate"><span class="pre">saliency_layer</span></code> and an <code class="xref py py-class docutils literal notranslate"><span class="pre">collections.OrderedDict</span></code>
of <code class="xref py py-class docutils literal notranslate"><span class="pre">Probe</span></code> objects for all modules in the model.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code> or tuple</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torchray.attribution.excitation_backprop.excitation_backprop">
<code class="sig-prename descclassname">torchray.attribution.excitation_backprop.</code><code class="sig-name descname">excitation_backprop</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">context_builder=&lt;class 'torchray.attribution.excitation_backprop.ExcitationBackpropContext'&gt;</em>, <em class="sig-param">gradient_to_saliency=&lt;function gradient_to_excitation_backprop_saliency&gt;</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchray/attribution/excitation_backprop.html#excitation_backprop"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchray.attribution.excitation_backprop.excitation_backprop" title="Permalink to this definition">¶</a></dt>
<dd><p>Excitation backprop.</p>
<p>The function takes the same arguments as <a class="reference internal" href="#torchray.attribution.common.saliency" title="torchray.attribution.common.saliency"><code class="xref py py-func docutils literal notranslate"><span class="pre">common.saliency()</span></code></a>, with
the defaults required to apply the Excitation backprop method, and supports
the same arguments and return values.</p>
</dd></dl>

<dl class="class">
<dt id="torchray.attribution.excitation_backprop.ExcitationBackpropContext">
<em class="property">class </em><code class="sig-prename descclassname">torchray.attribution.excitation_backprop.</code><code class="sig-name descname">ExcitationBackpropContext</code><span class="sig-paren">(</span><em class="sig-param">enable=True</em>, <em class="sig-param">debug=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchray/attribution/excitation_backprop.html#ExcitationBackpropContext"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchray.attribution.excitation_backprop.ExcitationBackpropContext" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Context to use Excitation Backpropagation rules.</p>
</dd></dl>

</div>
</div>
<div class="section" id="module-torchray.attribution.extremal_perturbation">
<span id="extremal-perturbation"></span><h2>Extremal perturbation<a class="headerlink" href="#module-torchray.attribution.extremal_perturbation" title="Permalink to this headline">¶</a></h2>
<p>This module provides an implementation of the <em>Extremal Perturbations</em> (EP)
method of <a class="reference internal" href="#ep" id="id8"><span>[EP]</span></a> for saliency visualization. The interface is given by
the <a class="reference internal" href="#torchray.attribution.extremal_perturbation.extremal_perturbation" title="torchray.attribution.extremal_perturbation.extremal_perturbation"><code class="xref py py-func docutils literal notranslate"><span class="pre">extremal_perturbation()</span></code></a> function:</p>
<div class="highlight-python notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchray.attribution.extremal_perturbation</span> <span class="kn">import</span> <span class="n">extremal_perturbation</span><span class="p">,</span> <span class="n">contrastive_reward</span>
<span class="kn">from</span> <span class="nn">torchray.benchmark</span> <span class="kn">import</span> <span class="n">get_example_data</span><span class="p">,</span> <span class="n">plot_example</span>
<span class="kn">from</span> <span class="nn">torchray.utils</span> <span class="kn">import</span> <span class="n">get_device</span>

<span class="c1"># Obtain example data.</span>
<span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">category_id_1</span><span class="p">,</span> <span class="n">category_id_2</span> <span class="o">=</span> <span class="n">get_example_data</span><span class="p">()</span>

<span class="c1"># Run on GPU if available.</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">get_device</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Extremal perturbation backprop.</span>
<span class="n">masks_1</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">extremal_perturbation</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">category_id_1</span><span class="p">,</span>
    <span class="n">reward_func</span><span class="o">=</span><span class="n">contrastive_reward</span><span class="p">,</span>
    <span class="n">debug</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">areas</span><span class="o">=</span><span class="p">[</span><span class="mf">0.12</span><span class="p">],</span>
<span class="p">)</span>

<span class="n">masks_2</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">extremal_perturbation</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">category_id_2</span><span class="p">,</span>
    <span class="n">reward_func</span><span class="o">=</span><span class="n">contrastive_reward</span><span class="p">,</span>
    <span class="n">debug</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">areas</span><span class="o">=</span><span class="p">[</span><span class="mf">0.05</span><span class="p">],</span>
<span class="p">)</span>

<span class="c1"># Plots.</span>
<span class="n">plot_example</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">masks_1</span><span class="p">,</span> <span class="s1">&#39;extremal perturbation&#39;</span><span class="p">,</span> <span class="n">category_id_1</span><span class="p">)</span>
<span class="n">plot_example</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">masks_2</span><span class="p">,</span> <span class="s1">&#39;extremal perturbation&#39;</span><span class="p">,</span> <span class="n">category_id_2</span><span class="p">)</span>
</pre></div>
</td></tr></table></div>
<p>Extremal perturbations seek to find a region of the input image that maximally
excites a certain output or intermediate activation of a neural network.</p>
<div class="section" id="perturbation-types">
<span id="ep-perturbations"></span><h3>Perturbation types<a class="headerlink" href="#perturbation-types" title="Permalink to this headline">¶</a></h3>
<p>The <a class="reference internal" href="#torchray.attribution.extremal_perturbation.Perturbation" title="torchray.attribution.extremal_perturbation.Perturbation"><code class="xref py py-class docutils literal notranslate"><span class="pre">Perturbation</span></code></a> class supports the following perturbation types:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#torchray.attribution.extremal_perturbation.BLUR_PERTURBATION" title="torchray.attribution.extremal_perturbation.BLUR_PERTURBATION"><code class="xref py py-attr docutils literal notranslate"><span class="pre">BLUR_PERTURBATION</span></code></a>: Gaussian blur.</p></li>
<li><p><a class="reference internal" href="#torchray.attribution.extremal_perturbation.FADE_PERTURBATION" title="torchray.attribution.extremal_perturbation.FADE_PERTURBATION"><code class="xref py py-attr docutils literal notranslate"><span class="pre">FADE_PERTURBATION</span></code></a>: Fade to black.</p></li>
</ul>
</div>
<div class="section" id="extremal-perturbation-variants">
<span id="ep-variants"></span><h3>Extremal perturbation variants<a class="headerlink" href="#extremal-perturbation-variants" title="Permalink to this headline">¶</a></h3>
<p>The <a class="reference internal" href="#torchray.attribution.extremal_perturbation.extremal_perturbation" title="torchray.attribution.extremal_perturbation.extremal_perturbation"><code class="xref py py-func docutils literal notranslate"><span class="pre">extremal_perturbation()</span></code></a> function supports the following variants:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#torchray.attribution.extremal_perturbation.PRESERVE_VARIANT" title="torchray.attribution.extremal_perturbation.PRESERVE_VARIANT"><code class="xref py py-attr docutils literal notranslate"><span class="pre">PRESERVE_VARIANT</span></code></a>: Find a mask that makes the activations large.</p></li>
<li><p><a class="reference internal" href="#torchray.attribution.extremal_perturbation.DELETE_VARIANT" title="torchray.attribution.extremal_perturbation.DELETE_VARIANT"><code class="xref py py-attr docutils literal notranslate"><span class="pre">DELETE_VARIANT</span></code></a>: Find a mask that makes the activations small.</p></li>
<li><p><a class="reference internal" href="#torchray.attribution.extremal_perturbation.DUAL_VARIANT" title="torchray.attribution.extremal_perturbation.DUAL_VARIANT"><code class="xref py py-attr docutils literal notranslate"><span class="pre">DUAL_VARIANT</span></code></a>: Find a mask that makes the activations large and whose
complement makes the activations small, rewarding the difference between
these two.</p></li>
</ul>
<p class="rubric">References</p>
<dl class="citation">
<dt class="label" id="ep"><span class="brackets"><a class="fn-backref" href="#id8">EP</a></span></dt>
<dd><p>Ruth C. Fong, Mandela Patrick and Andrea Vedaldi,
<em>Understanding Deep Networks via Extremal Perturbations and Smooth Masks,</em>
ICCV 2019,
<a class="reference external" href="http://arxiv.org/">http://arxiv.org/</a>.</p>
</dd>
</dl>
<dl class="function">
<dt id="torchray.attribution.extremal_perturbation.extremal_perturbation">
<code class="sig-prename descclassname">torchray.attribution.extremal_perturbation.</code><code class="sig-name descname">extremal_perturbation</code><span class="sig-paren">(</span><em class="sig-param">model, input, target, areas=[0.1], perturbation='blur', max_iter=800, num_levels=8, step=7, sigma=21, jitter=True, variant='preserve', print_iter=None, debug=False, reward_func=&lt;function simple_reward&gt;, resize=False, resize_mode='bilinear', smooth=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchray/attribution/extremal_perturbation.html#extremal_perturbation"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchray.attribution.extremal_perturbation.extremal_perturbation" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute a set of extremal perturbations.</p>
<p>The function takes a <code class="xref py py-attr docutils literal notranslate"><span class="pre">model</span></code>, an <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor <span class="math notranslate nohighlight">\(x\)</span>
of size <span class="math notranslate nohighlight">\(1\times C\times H\times W\)</span>, and a <code class="xref py py-attr docutils literal notranslate"><span class="pre">target</span></code>
activation channel. It produces as output a
<span class="math notranslate nohighlight">\(K\times C\times H\times W\)</span> tensor where <span class="math notranslate nohighlight">\(K\)</span> is the number
of specified <code class="xref py py-attr docutils literal notranslate"><span class="pre">areas</span></code>.</p>
<p>Each mask, which has approximately the specified area, is searched
in order to maximise the (spatial average of the) activations
in channel <code class="xref py py-attr docutils literal notranslate"><span class="pre">target</span></code>. Alternative objectives can be specified
via <code class="xref py py-attr docutils literal notranslate"><span class="pre">reward_func</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>) – model.</p></li>
<li><p><strong>input</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code>) – input tensor.</p></li>
<li><p><strong>target</strong> (<em>int</em>) – target channel.</p></li>
<li><p><strong>areas</strong> (<em>float</em><em> or </em><em>list of floats</em><em>, </em><em>optional</em>) – list of target areas for saliency
masks. Defaults to <cite>[0.1]</cite>.</p></li>
<li><p><strong>perturbation</strong> (<em>str</em><em>, </em><em>optional</em>) – <a class="reference internal" href="#ep-perturbations"><span class="std std-ref">Perturbation types</span></a>.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>optional</em>) – number of iterations for optimizing the masks.</p></li>
<li><p><strong>num_levels</strong> (<em>int</em><em>, </em><em>optional</em>) – number of buckets with which to discretize
and linearly interpolate the perturbation
(see <a class="reference internal" href="#torchray.attribution.extremal_perturbation.Perturbation" title="torchray.attribution.extremal_perturbation.Perturbation"><code class="xref py py-class docutils literal notranslate"><span class="pre">Perturbation</span></code></a>). Defaults to 8.</p></li>
<li><p><strong>step</strong> (<em>int</em><em>, </em><em>optional</em>) – mask step (see <code class="xref py py-class docutils literal notranslate"><span class="pre">MaskGenerator</span></code>).
Defaults to 7.</p></li>
<li><p><strong>sigma</strong> (<em>float</em><em>, </em><em>optional</em>) – mask smoothing (see <code class="xref py py-class docutils literal notranslate"><span class="pre">MaskGenerator</span></code>).
Defaults to 21.</p></li>
<li><p><strong>jitter</strong> (<em>bool</em><em>, </em><em>optional</em>) – randomly flip the image horizontally at each iteration.
Defaults to True.</p></li>
<li><p><strong>variant</strong> (<em>str</em><em>, </em><em>optional</em>) – <a class="reference internal" href="#ep-variants"><span class="std std-ref">Extremal perturbation variants</span></a>. Defaults to
<a class="reference internal" href="#torchray.attribution.extremal_perturbation.PRESERVE_VARIANT" title="torchray.attribution.extremal_perturbation.PRESERVE_VARIANT"><code class="xref py py-attr docutils literal notranslate"><span class="pre">PRESERVE_VARIANT</span></code></a>.</p></li>
<li><p><strong>print_iter</strong> (<em>int</em><em>, </em><em>optional</em>) – frequency with which to print losses.
Defaults to None.</p></li>
<li><p><strong>debug</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, generate debug plots.</p></li>
<li><p><strong>reward_func</strong> (<em>function</em><em>, </em><em>optional</em>) – function that generates reward tensor
to backpropagate.</p></li>
<li><p><strong>resize</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, upsamples the masks the same size
as <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>. It is also possible to specify a pair
(width, height) for a different size. Defaults to False.</p></li>
<li><p><strong>resize_mode</strong> (<em>str</em><em>, </em><em>optional</em>) – Upsampling method to use. Defaults to
<code class="docutils literal notranslate"><span class="pre">'bilinear'</span></code>.</p></li>
<li><p><strong>smooth</strong> (<em>float</em><em>, </em><em>optional</em>) – Apply Gaussian smoothing to the masks after
computing them. Defaults to 0.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tuple containing the masks and the energies.
The masks are stored as a <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code>
of dimension</p>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torchray.attribution.extremal_perturbation.Perturbation">
<em class="property">class </em><code class="sig-prename descclassname">torchray.attribution.extremal_perturbation.</code><code class="sig-name descname">Perturbation</code><span class="sig-paren">(</span><em class="sig-param">input</em>, <em class="sig-param">num_levels=8</em>, <em class="sig-param">max_blur=20</em>, <em class="sig-param">type='blur'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchray/attribution/extremal_perturbation.html#Perturbation"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchray.attribution.extremal_perturbation.Perturbation" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Perturbation pyramid.</p>
<p>The class takes as input a tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and applies to it
perturbation of increasing strenght, storing the resulting pyramid as
the class state. The method <a class="reference internal" href="#torchray.attribution.extremal_perturbation.Perturbation.apply" title="torchray.attribution.extremal_perturbation.Perturbation.apply"><code class="xref py py-func docutils literal notranslate"><span class="pre">apply()</span></code></a> can then be used to generate an
inhomogeneously perturbed image based on a certain perturbation mask.</p>
<p>The pyramid <span class="math notranslate nohighlight">\(y\)</span> is the <span class="math notranslate nohighlight">\(L\times C\times H\times W\)</span> tensor</p>
<div class="math notranslate nohighlight">
\[y_{lcvu} = [\operatorname{perturb}(x, \sigma_l)]_{cvu}\]</div>
<p>where <span class="math notranslate nohighlight">\(x\)</span> is the input tensor, <span class="math notranslate nohighlight">\(c\)</span> a channel, <span class="math notranslate nohighlight">\(vu\)</span>,
the spatial location, <span class="math notranslate nohighlight">\(l\)</span> a perturbation level,  and
<span class="math notranslate nohighlight">\(\operatorname{perturb}\)</span> is a perturbation operator.</p>
<p>For the <em>blur perturbation</em> (<a class="reference internal" href="#torchray.attribution.extremal_perturbation.BLUR_PERTURBATION" title="torchray.attribution.extremal_perturbation.BLUR_PERTURBATION"><code class="xref py py-attr docutils literal notranslate"><span class="pre">BLUR_PERTURBATION</span></code></a>), the perturbation
operator amounts to convolution with a Gaussian whose kernel has
standard deviation <span class="math notranslate nohighlight">\(\sigma_l = \sigma_{\mathrm{max}} (1 -  l/ (L-1))\)</span>:</p>
<div class="math notranslate nohighlight">
\[\operatorname{perturb}(x, \sigma_l) = g_{\sigma_l} \ast x\]</div>
<p>For the <em>fade perturbation</em> (<a class="reference internal" href="#torchray.attribution.extremal_perturbation.FADE_PERTURBATION" title="torchray.attribution.extremal_perturbation.FADE_PERTURBATION"><code class="xref py py-attr docutils literal notranslate"><span class="pre">FADE_PERTURBATION</span></code></a>),</p>
<div class="math notranslate nohighlight">
\[\operatorname{perturb}(x, \sigma_l) = \sigma_l \cdot x\]</div>
<p>where  <span class="math notranslate nohighlight">\(\sigma_l =  l / (L-1)\)</span>.</p>
<p>Note that in all cases the last pyramid level <span class="math notranslate nohighlight">\(l=L-1\)</span> corresponds
to the unperturbed input and the first <span class="math notranslate nohighlight">\(l=0\)</span> to the maximally
perturbed input.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code>) – A <span class="math notranslate nohighlight">\(1\times C\times H\times W\)</span>
input tensor (usually an image).</p></li>
<li><p><strong>num_levels</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of pyramid leves. Defaults to 8.</p></li>
<li><p><strong>type</strong> (<em>str</em><em>, </em><em>optional</em>) – Perturbation type (<a class="reference internal" href="#ep-perturbations"><span class="std std-ref">Perturbation types</span></a>).</p></li>
<li><p><strong>max_blur</strong> (<em>float</em><em>, </em><em>optional</em>) – <span class="math notranslate nohighlight">\(\sigma_{\mathrm{max}}\)</span> for the
Gaussian blur perturbation. Defaults to 20.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="torchray.attribution.extremal_perturbation.Perturbation.pyramid">
<code class="sig-name descname">pyramid</code><a class="headerlink" href="#torchray.attribution.extremal_perturbation.Perturbation.pyramid" title="Permalink to this definition">¶</a></dt>
<dd><p>A <span class="math notranslate nohighlight">\(L\times C\times H\times W\)</span>
tensor with <span class="math notranslate nohighlight">\(L\)</span> ():attr:<cite>num_levels</cite>) increasingly
perturbed versions of the input tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torchray.attribution.extremal_perturbation.Perturbation.apply">
<code class="sig-name descname">apply</code><span class="sig-paren">(</span><em class="sig-param">mask</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchray/attribution/extremal_perturbation.html#Perturbation.apply"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchray.attribution.extremal_perturbation.Perturbation.apply" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate a perturbetd tensor from a perturbation mask.</p>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> is a tensor <span class="math notranslate nohighlight">\(K\times 1\times H\times W\)</span>
with spatial dimensions <span class="math notranslate nohighlight">\(H\times W\)</span> matching the input
tensor passed upon instantiation of the class. The output
is a <span class="math notranslate nohighlight">\(K\times C\times H\times W\)</span> with <span class="math notranslate nohighlight">\(K\)</span> perturbed
versions of the input tensor, one for each mask.</p>
<p>Masks values are in the range 0 to 1, where 1 means that the input
tensor is copied as is, and 0 that it is maximally perturbed.</p>
<p>Formally, the output is then given by:</p>
<div class="math notranslate nohighlight">
\[z_{kcvu} = y_{m_{k1cu}, c, v, u}\]</div>
<p>where <span class="math notranslate nohighlight">\(k\)</span> index the mask, <span class="math notranslate nohighlight">\(c\)</span> the feature channel,
<span class="math notranslate nohighlight">\(vu\)</span> the spatial location, <span class="math notranslate nohighlight">\(y\)</span> is the pyramid tensor,
and <span class="math notranslate nohighlight">\(m\)</span> the mask tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code>.</p>
<p>The mask must be in the range <span class="math notranslate nohighlight">\([0, 1]\)</span>. Linear interpolation
is used to index the perturbation level dimension of <span class="math notranslate nohighlight">\(y\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>mask</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code>) – A <span class="math notranslate nohighlight">\(K\times 1\times H\times W\)</span>
input tensor representing <span class="math notranslate nohighlight">\(K\)</span> masks.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <span class="math notranslate nohighlight">\(K\times C\times H\times W\)</span> tensor
with <span class="math notranslate nohighlight">\(K\)</span> perturbed versions of the input tensor.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torchray.attribution.extremal_perturbation.Perturbation.to">
<code class="sig-name descname">to</code><span class="sig-paren">(</span><em class="sig-param">dev</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchray/attribution/extremal_perturbation.html#Perturbation.to"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchray.attribution.extremal_perturbation.Perturbation.to" title="Permalink to this definition">¶</a></dt>
<dd><p>Switch to another device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dev</strong> – PyTorch device.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#torchray.attribution.extremal_perturbation.Perturbation" title="torchray.attribution.extremal_perturbation.Perturbation">Perturbation</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="torchray.attribution.extremal_perturbation.simple_reward">
<code class="sig-prename descclassname">torchray.attribution.extremal_perturbation.</code><code class="sig-name descname">simple_reward</code><span class="sig-paren">(</span><em class="sig-param">activation</em>, <em class="sig-param">target</em>, <em class="sig-param">variant</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchray/attribution/extremal_perturbation.html#simple_reward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchray.attribution.extremal_perturbation.simple_reward" title="Permalink to this definition">¶</a></dt>
<dd><p>Simple reward.</p>
<p>For the <a class="reference internal" href="#torchray.attribution.extremal_perturbation.PRESERVE_VARIANT" title="torchray.attribution.extremal_perturbation.PRESERVE_VARIANT"><code class="xref py py-attr docutils literal notranslate"><span class="pre">PRESERVE_VARIANT</span></code></a>, the simple reward is given by:</p>
<div class="math notranslate nohighlight">
\[z_{k1vu} = y_{n, c, v, u}\]</div>
<p>where <span class="math notranslate nohighlight">\(y\)</span> is the <span class="math notranslate nohighlight">\(K\times C\times H\times W\)</span> <code class="xref py py-attr docutils literal notranslate"><span class="pre">activation</span></code>
tensor, <span class="math notranslate nohighlight">\(c\)</span> the <code class="xref py py-attr docutils literal notranslate"><span class="pre">target</span></code> channel, <span class="math notranslate nohighlight">\(k\)</span> the mask index
and <span class="math notranslate nohighlight">\(vu\)</span> the spatial indices. <span class="math notranslate nohighlight">\(c\)</span> must be in the range
<span class="math notranslate nohighlight">\([0, C-1]\)</span>.</p>
<p>For the <a class="reference internal" href="#torchray.attribution.extremal_perturbation.DELETE_VARIANT" title="torchray.attribution.extremal_perturbation.DELETE_VARIANT"><code class="xref py py-attr docutils literal notranslate"><span class="pre">DELETE_VARIANT</span></code></a>, the reward is the opposite.</p>
<p>For the <a class="reference internal" href="#torchray.attribution.extremal_perturbation.DUAL_VARIANT" title="torchray.attribution.extremal_perturbation.DUAL_VARIANT"><code class="xref py py-attr docutils literal notranslate"><span class="pre">DUAL_VARIANT</span></code></a>, it is given by:</p>
<div class="math notranslate nohighlight">
\[z_{n1vu} = y_{n, c, v, u} - y_{n + N/2, c, v, u}.\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>activation</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code>) – activation tensor.</p></li>
<li><p><strong>target</strong> (<em>int</em>) – target channel.</p></li>
<li><p><strong>variant</strong> (<em>str</em>) – A <a class="reference internal" href="#ep-variants"><span class="std std-ref">Extremal perturbation variants</span></a>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>reward tensor with the same shape as
<code class="xref py py-attr docutils literal notranslate"><span class="pre">activation</span></code> but a single channel.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torchray.attribution.extremal_perturbation.contrastive_reward">
<code class="sig-prename descclassname">torchray.attribution.extremal_perturbation.</code><code class="sig-name descname">contrastive_reward</code><span class="sig-paren">(</span><em class="sig-param">activation</em>, <em class="sig-param">target</em>, <em class="sig-param">variant</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchray/attribution/extremal_perturbation.html#contrastive_reward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchray.attribution.extremal_perturbation.contrastive_reward" title="Permalink to this definition">¶</a></dt>
<dd><p>Contrastive reward.</p>
<p>For the <a class="reference internal" href="#torchray.attribution.extremal_perturbation.PRESERVE_VARIANT" title="torchray.attribution.extremal_perturbation.PRESERVE_VARIANT"><code class="xref py py-attr docutils literal notranslate"><span class="pre">PRESERVE_VARIANT</span></code></a>, the contrastive reward is given by:</p>
<div class="math notranslate nohighlight">
\[z_{k1vu} = y_{n, c, v, u} - \max_{c'\not= c} y_{n, c', v, u}\]</div>
<p>The other variants are derived in the same manner as for
<a class="reference internal" href="#torchray.attribution.extremal_perturbation.simple_reward" title="torchray.attribution.extremal_perturbation.simple_reward"><code class="xref py py-func docutils literal notranslate"><span class="pre">simple_reward()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>activation</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code>) – activation tensor.</p></li>
<li><p><strong>target</strong> (<em>int</em>) – target channel.</p></li>
<li><p><strong>variant</strong> (<em>str</em>) – A <a class="reference internal" href="#ep-variants"><span class="std std-ref">Extremal perturbation variants</span></a>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>reward tensor with the same shape as
<code class="xref py py-attr docutils literal notranslate"><span class="pre">activation</span></code> but a single channel.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="data">
<dt id="torchray.attribution.extremal_perturbation.BLUR_PERTURBATION">
<code class="sig-prename descclassname">torchray.attribution.extremal_perturbation.</code><code class="sig-name descname">BLUR_PERTURBATION</code><em class="property"> = 'blur'</em><a class="headerlink" href="#torchray.attribution.extremal_perturbation.BLUR_PERTURBATION" title="Permalink to this definition">¶</a></dt>
<dd><p>Blur-type perturbation for <a class="reference internal" href="#torchray.attribution.extremal_perturbation.Perturbation" title="torchray.attribution.extremal_perturbation.Perturbation"><code class="xref py py-class docutils literal notranslate"><span class="pre">Perturbation</span></code></a>.</p>
</dd></dl>

<dl class="data">
<dt id="torchray.attribution.extremal_perturbation.FADE_PERTURBATION">
<code class="sig-prename descclassname">torchray.attribution.extremal_perturbation.</code><code class="sig-name descname">FADE_PERTURBATION</code><em class="property"> = 'fade'</em><a class="headerlink" href="#torchray.attribution.extremal_perturbation.FADE_PERTURBATION" title="Permalink to this definition">¶</a></dt>
<dd><p>Fade-type perturbation for <a class="reference internal" href="#torchray.attribution.extremal_perturbation.Perturbation" title="torchray.attribution.extremal_perturbation.Perturbation"><code class="xref py py-class docutils literal notranslate"><span class="pre">Perturbation</span></code></a>.</p>
</dd></dl>

<dl class="data">
<dt id="torchray.attribution.extremal_perturbation.PRESERVE_VARIANT">
<code class="sig-prename descclassname">torchray.attribution.extremal_perturbation.</code><code class="sig-name descname">PRESERVE_VARIANT</code><em class="property"> = 'preserve'</em><a class="headerlink" href="#torchray.attribution.extremal_perturbation.PRESERVE_VARIANT" title="Permalink to this definition">¶</a></dt>
<dd><p>Preservation game for <a class="reference internal" href="#torchray.attribution.extremal_perturbation.extremal_perturbation" title="torchray.attribution.extremal_perturbation.extremal_perturbation"><code class="xref py py-func docutils literal notranslate"><span class="pre">extremal_perturbation()</span></code></a>.</p>
</dd></dl>

<dl class="data">
<dt id="torchray.attribution.extremal_perturbation.DELETE_VARIANT">
<code class="sig-prename descclassname">torchray.attribution.extremal_perturbation.</code><code class="sig-name descname">DELETE_VARIANT</code><em class="property"> = 'delete'</em><a class="headerlink" href="#torchray.attribution.extremal_perturbation.DELETE_VARIANT" title="Permalink to this definition">¶</a></dt>
<dd><p>Deletion game for <a class="reference internal" href="#torchray.attribution.extremal_perturbation.extremal_perturbation" title="torchray.attribution.extremal_perturbation.extremal_perturbation"><code class="xref py py-func docutils literal notranslate"><span class="pre">extremal_perturbation()</span></code></a>.</p>
</dd></dl>

<dl class="data">
<dt id="torchray.attribution.extremal_perturbation.DUAL_VARIANT">
<code class="sig-prename descclassname">torchray.attribution.extremal_perturbation.</code><code class="sig-name descname">DUAL_VARIANT</code><em class="property"> = 'dual'</em><a class="headerlink" href="#torchray.attribution.extremal_perturbation.DUAL_VARIANT" title="Permalink to this definition">¶</a></dt>
<dd><p>Combined game for <a class="reference internal" href="#torchray.attribution.extremal_perturbation.extremal_perturbation" title="torchray.attribution.extremal_perturbation.extremal_perturbation"><code class="xref py py-func docutils literal notranslate"><span class="pre">extremal_perturbation()</span></code></a>.</p>
</dd></dl>

</div>
</div>
<div class="section" id="module-torchray.attribution.gradient">
<span id="gradient"></span><h2>Gradient<a class="headerlink" href="#module-torchray.attribution.gradient" title="Permalink to this headline">¶</a></h2>
<p>This module implements the <em>gradient</em> method of <a class="reference internal" href="#grad" id="id9"><span>[GRAD]</span></a> for visualizing a deep
network. It is a backpropagation method, and in fact the simplest of them all
as it coincides with standard backpropagation. The simplest way to use this
method is via the <a class="reference internal" href="#torchray.attribution.gradient.gradient" title="torchray.attribution.gradient.gradient"><code class="xref py py-func docutils literal notranslate"><span class="pre">gradient()</span></code></a> function:</p>
<div class="highlight-python notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchray.attribution.gradient</span> <span class="kn">import</span> <span class="n">gradient</span>
<span class="kn">from</span> <span class="nn">torchray.benchmark</span> <span class="kn">import</span> <span class="n">get_example_data</span><span class="p">,</span> <span class="n">plot_example</span>

<span class="c1"># Obtain example data.</span>
<span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">category_id</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">get_example_data</span><span class="p">()</span>

<span class="c1"># Gradient method.</span>
<span class="n">saliency</span> <span class="o">=</span> <span class="n">gradient</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">category_id</span><span class="p">)</span>

<span class="c1"># Plots.</span>
<span class="n">plot_example</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">saliency</span><span class="p">,</span> <span class="s1">&#39;gradient&#39;</span><span class="p">,</span> <span class="n">category_id</span><span class="p">)</span>
</pre></div>
</td></tr></table></div>
<p>Alternatively, one can do so manually, as follows</p>
<div class="highlight-python notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchray.attribution.common</span> <span class="kn">import</span> <span class="n">gradient_to_saliency</span>
<span class="kn">from</span> <span class="nn">torchray.benchmark</span> <span class="kn">import</span> <span class="n">get_example_data</span><span class="p">,</span> <span class="n">plot_example</span>

<span class="c1"># Obtain example data.</span>
<span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">category_id</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">get_example_data</span><span class="p">()</span>

<span class="c1"># Gradient method.</span>

<span class="n">x</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">category_id</span><span class="p">]</span>
<span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="n">saliency</span> <span class="o">=</span> <span class="n">gradient_to_saliency</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Plots.</span>
<span class="n">plot_example</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">saliency</span><span class="p">,</span> <span class="s1">&#39;gradient&#39;</span><span class="p">,</span> <span class="n">category_id</span><span class="p">)</span>
</pre></div>
</td></tr></table></div>
<p>Note that in this example, for visualization, the gradient is
convernted into an image by postprocessing by using the function
<a class="reference internal" href="#torchray.attribution.common.saliency" title="torchray.attribution.common.saliency"><code class="xref py py-func docutils literal notranslate"><span class="pre">torchray.attribution.common.saliency()</span></code></a>.</p>
<p>See also <a class="reference internal" href="#backprop"><span class="std std-ref">Attribution</span></a> for further examples.</p>
<p class="rubric">References</p>
<dl class="citation">
<dt class="label" id="grad"><span class="brackets"><a class="fn-backref" href="#id9">GRAD</a></span></dt>
<dd><p>Karen Simonyan, Andrea Vedaldi and Andrew Zisserman,
<em>Deep Inside Convolutional Networks:
Visualising Image Classification Models and Saliency Maps,</em>
ICLR workshop, 2014,
<a class="reference external" href="https://arxiv.org/abs/1312.6034">https://arxiv.org/abs/1312.6034</a>.</p>
</dd>
</dl>
<dl class="function">
<dt id="torchray.attribution.gradient.gradient">
<code class="sig-prename descclassname">torchray.attribution.gradient.</code><code class="sig-name descname">gradient</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">context_builder=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchray/attribution/gradient.html#gradient"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchray.attribution.gradient.gradient" title="Permalink to this definition">¶</a></dt>
<dd><p>Gradient method</p>
<p>The function takes the same arguments as <a class="reference internal" href="#torchray.attribution.common.saliency" title="torchray.attribution.common.saliency"><code class="xref py py-func docutils literal notranslate"><span class="pre">common.saliency()</span></code></a>, with
the defaults required to apply the gradient method, and supports the
same arguments and return values.</p>
</dd></dl>

</div>
<div class="section" id="module-torchray.attribution.grad_cam">
<span id="grad-cam"></span><h2>Grad-CAM<a class="headerlink" href="#module-torchray.attribution.grad_cam" title="Permalink to this headline">¶</a></h2>
<p>This module provides an implementation of the <em>Grad-CAM</em> method of <a class="reference internal" href="#gradcam" id="id10"><span>[GRADCAM]</span></a>
for saliency visualization. The simplest interface is given by the
<a class="reference internal" href="#torchray.attribution.grad_cam.grad_cam" title="torchray.attribution.grad_cam.grad_cam"><code class="xref py py-func docutils literal notranslate"><span class="pre">grad_cam()</span></code></a> function:</p>
<div class="highlight-python notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchray.attribution.grad_cam</span> <span class="kn">import</span> <span class="n">grad_cam</span>
<span class="kn">from</span> <span class="nn">torchray.benchmark</span> <span class="kn">import</span> <span class="n">get_example_data</span><span class="p">,</span> <span class="n">plot_example</span>

<span class="c1"># Obtain example data.</span>
<span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">category_id</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">get_example_data</span><span class="p">()</span>

<span class="c1"># Grad-CAM backprop.</span>
<span class="n">saliency</span> <span class="o">=</span> <span class="n">grad_cam</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">category_id</span><span class="p">,</span> <span class="n">saliency_layer</span><span class="o">=</span><span class="s1">&#39;features.29&#39;</span><span class="p">)</span>

<span class="c1"># Plots.</span>
<span class="n">plot_example</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">saliency</span><span class="p">,</span> <span class="s1">&#39;grad-cam backprop&#39;</span><span class="p">,</span> <span class="n">category_id</span><span class="p">)</span>
</pre></div>
</td></tr></table></div>
<p>Alternatively, it is possible to run the method “manually”. Grad-CAM backprop
is a variant of the gradient method, applied at an intermediate layer:</p>
<div class="highlight-python notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchray.attribution.common</span> <span class="kn">import</span> <span class="n">Probe</span><span class="p">,</span> <span class="n">get_module</span>
<span class="kn">from</span> <span class="nn">torchray.attribution.grad_cam</span> <span class="kn">import</span> <span class="n">gradient_to_grad_cam_saliency</span>
<span class="kn">from</span> <span class="nn">torchray.benchmark</span> <span class="kn">import</span> <span class="n">get_example_data</span><span class="p">,</span> <span class="n">plot_example</span>

<span class="c1"># Obtain example data.</span>
<span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">category_id</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">get_example_data</span><span class="p">()</span>

<span class="c1"># Grad-CAM backprop.</span>
<span class="n">saliency_layer</span> <span class="o">=</span> <span class="n">get_module</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;features.29&#39;</span><span class="p">)</span>

<span class="n">probe</span> <span class="o">=</span> <span class="n">Probe</span><span class="p">(</span><span class="n">saliency_layer</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="s1">&#39;output&#39;</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">category_id</span><span class="p">]</span>
<span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="n">saliency</span> <span class="o">=</span> <span class="n">gradient_to_grad_cam_saliency</span><span class="p">(</span><span class="n">probe</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="c1"># Plots.</span>
<span class="n">plot_example</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">saliency</span><span class="p">,</span> <span class="s1">&#39;grad-cam backprop&#39;</span><span class="p">,</span> <span class="n">category_id</span><span class="p">)</span>
</pre></div>
</td></tr></table></div>
<p>Note that the function <code class="xref py py-func docutils literal notranslate"><span class="pre">gradient_to_grad_cam_saliency()</span></code> is used to convert
activations and gradients to a saliency map.</p>
<p>See also <a class="reference internal" href="#backprop"><span class="std std-ref">Attribution</span></a> for further examples and discussion.</p>
<div class="section" id="id11">
<h3>Theory<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h3>
<p>Grad-CAM can be seen as a variant of the <em>gradient</em> method
(<a class="reference internal" href="#module-torchray.attribution.gradient" title="torchray.attribution.gradient"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torchray.attribution.gradient</span></code></a>) with two differences:</p>
<ol class="arabic simple">
<li><p>The saliency is measured at an intermediate layer of the network, usually at
the output of the last convolutional layer.</p></li>
<li><p>Saliency is defined as the clamped product of forward activation and
backward gradient at that layer.</p></li>
</ol>
<p class="rubric">References</p>
<dl class="citation">
<dt class="label" id="gradcam"><span class="brackets"><a class="fn-backref" href="#id10">GRADCAM</a></span></dt>
<dd><p>Ramprasaath R. Selvaraju, Abhishek Das, Ramakrishna Vedantam,
Michael Cogswell, Devi Parikh and Dhruv Batra,
<em>Visual Explanations from Deep Networks via Gradient-based
Localization,</em>
ICCV 2017,
<a class="reference external" href="http://openaccess.thecvf.com/content_iccv_2017/html/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.html">http://openaccess.thecvf.com/content_iccv_2017/html/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.html</a>.</p>
</dd>
</dl>
<dl class="function">
<dt id="torchray.attribution.grad_cam.grad_cam">
<code class="sig-prename descclassname">torchray.attribution.grad_cam.</code><code class="sig-name descname">grad_cam</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">saliency_layer</em>, <em class="sig-param">gradient_to_saliency=&lt;function gradient_to_grad_cam_saliency&gt;</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchray/attribution/grad_cam.html#grad_cam"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchray.attribution.grad_cam.grad_cam" title="Permalink to this definition">¶</a></dt>
<dd><p>Grad-CAM method.</p>
<p>The function takes the same arguments as <a class="reference internal" href="#torchray.attribution.common.saliency" title="torchray.attribution.common.saliency"><code class="xref py py-func docutils literal notranslate"><span class="pre">common.saliency()</span></code></a>, with
the defaults required to apply the Grad-CAM method, and supports the
same arguments and return values.</p>
</dd></dl>

</div>
</div>
<div class="section" id="module-torchray.attribution.guided_backprop">
<span id="guided-backprop"></span><h2>Guided backprop<a class="headerlink" href="#module-torchray.attribution.guided_backprop" title="Permalink to this headline">¶</a></h2>
<p>This module implements <em>guided backpropagation</em> method of <a class="reference internal" href="#guided" id="id12"><span>[GUIDED]</span></a> or
visualizing deep networks. The simplest interface is given by the
<a class="reference internal" href="#torchray.attribution.guided_backprop.guided_backprop" title="torchray.attribution.guided_backprop.guided_backprop"><code class="xref py py-func docutils literal notranslate"><span class="pre">guided_backprop()</span></code></a> function:</p>
<div class="highlight-python notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchray.attribution.guided_backprop</span> <span class="kn">import</span> <span class="n">guided_backprop</span>
<span class="kn">from</span> <span class="nn">torchray.benchmark</span> <span class="kn">import</span> <span class="n">get_example_data</span><span class="p">,</span> <span class="n">plot_example</span>

<span class="c1"># Obtain example data.</span>
<span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">category_id</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">get_example_data</span><span class="p">()</span>

<span class="c1"># Guided backprop.</span>
<span class="n">saliency</span> <span class="o">=</span> <span class="n">guided_backprop</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">category_id</span><span class="p">)</span>

<span class="c1"># Plots.</span>
<span class="n">plot_example</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">saliency</span><span class="p">,</span> <span class="s1">&#39;guided backprop&#39;</span><span class="p">,</span> <span class="n">category_id</span><span class="p">)</span>
</pre></div>
</td></tr></table></div>
<p>Alternatively, it is possible to run the method “manually”. Guided backprop is
a backpropagation method, and thus works by changing the definition of the
backward functions of some layers.  This can be done using the
<a class="reference internal" href="#torchray.attribution.guided_backprop.GuidedBackpropContext" title="torchray.attribution.guided_backprop.GuidedBackpropContext"><code class="xref py py-class docutils literal notranslate"><span class="pre">GuidedBackpropContext</span></code></a> context:</p>
<div class="highlight-python notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchray.attribution.common</span> <span class="kn">import</span> <span class="n">gradient_to_saliency</span>
<span class="kn">from</span> <span class="nn">torchray.attribution.guided_backprop</span> <span class="kn">import</span> <span class="n">GuidedBackpropContext</span>
<span class="kn">from</span> <span class="nn">torchray.benchmark</span> <span class="kn">import</span> <span class="n">get_example_data</span><span class="p">,</span> <span class="n">plot_example</span>

<span class="c1"># Obtain example data.</span>
<span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">category_id</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">get_example_data</span><span class="p">()</span>

<span class="c1"># Guided backprop.</span>
<span class="n">x</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>

<span class="k">with</span> <span class="n">GuidedBackpropContext</span><span class="p">():</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">category_id</span><span class="p">]</span>
    <span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="n">saliency</span> <span class="o">=</span> <span class="n">gradient_to_saliency</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Plots.</span>
<span class="n">plot_example</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">saliency</span><span class="p">,</span> <span class="s1">&#39;guided backprop&#39;</span><span class="p">,</span> <span class="n">category_id</span><span class="p">)</span>
</pre></div>
</td></tr></table></div>
<p>See also <a class="reference internal" href="#backprop"><span class="std std-ref">Attribution</span></a> for further examples.</p>
<div class="section" id="id13">
<h3>Theory<a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h3>
<p>Guided backprop is a backpropagation method, and thus it works by changing the
definition of the backward functions of some layers. The only change is a
modified definition of the backward ReLU function:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\operatorname{ReLU}^*(x,p) =
\begin{cases}
p, &amp; \mathrm{if}~p &gt; 0 ~\mathrm{and}~ x &gt; 0,\\
0, &amp; \mathrm{otherwise} \\
\end{cases}\end{split}\]</div>
<p>The modified ReLU is implemented by class <code class="xref py py-class docutils literal notranslate"><span class="pre">GuidedBackpropReLU</span></code>.</p>
<p class="rubric">References</p>
<dl class="citation">
<dt class="label" id="guided"><span class="brackets"><a class="fn-backref" href="#id12">GUIDED</a></span></dt>
<dd><p>Springenberg et al.,
<em>Striving for simplicity: The all convolutional net</em>,
ICLR Workshop 2015,
<a class="reference external" href="https://arxiv.org/abs/1412.6806">https://arxiv.org/abs/1412.6806</a>.</p>
</dd>
</dl>
<dl class="class">
<dt id="torchray.attribution.guided_backprop.GuidedBackpropContext">
<em class="property">class </em><code class="sig-prename descclassname">torchray.attribution.guided_backprop.</code><code class="sig-name descname">GuidedBackpropContext</code><a class="reference internal" href="_modules/torchray/attribution/guided_backprop.html#GuidedBackpropContext"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchray.attribution.guided_backprop.GuidedBackpropContext" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchray.attribution.common.ReLUContext" title="torchray.attribution.common.ReLUContext"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchray.attribution.common.ReLUContext</span></code></a></p>
<p>GuidedBackprop context.</p>
<p>This context modifies the computation of gradients
to match the guided backpropagaton definition.</p>
<p>See <a class="reference internal" href="#module-torchray.attribution.guided_backprop" title="torchray.attribution.guided_backprop"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torchray.attribution.guided_backprop</span></code></a> for how to use it.</p>
</dd></dl>

<dl class="function">
<dt id="torchray.attribution.guided_backprop.guided_backprop">
<code class="sig-prename descclassname">torchray.attribution.guided_backprop.</code><code class="sig-name descname">guided_backprop</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">context_builder=&lt;class 'torchray.attribution.guided_backprop.GuidedBackpropContext'&gt;</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchray/attribution/guided_backprop.html#guided_backprop"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchray.attribution.guided_backprop.guided_backprop" title="Permalink to this definition">¶</a></dt>
<dd><p>Guided backprop.</p>
<p>The function takes the same arguments as <a class="reference internal" href="#torchray.attribution.common.saliency" title="torchray.attribution.common.saliency"><code class="xref py py-func docutils literal notranslate"><span class="pre">common.saliency()</span></code></a>, with
the defaults required to apply the guided backprop method, and supports the
same arguments and return values.</p>
</dd></dl>

</div>
</div>
<div class="section" id="module-torchray.attribution.linear_approx">
<span id="linear-approximation"></span><h2>Linear approximation<a class="headerlink" href="#module-torchray.attribution.linear_approx" title="Permalink to this headline">¶</a></h2>
<p>This module provides an implementation of the <em>linear approximation</em> method
for saliency visualization. The simplest interface is given by the
<a class="reference internal" href="#torchray.attribution.linear_approx.linear_approx" title="torchray.attribution.linear_approx.linear_approx"><code class="xref py py-func docutils literal notranslate"><span class="pre">linear_approx()</span></code></a> function:</p>
<div class="highlight-python notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchray.attribution.linear_approx</span> <span class="kn">import</span> <span class="n">linear_approx</span>
<span class="kn">from</span> <span class="nn">torchray.benchmark</span> <span class="kn">import</span> <span class="n">get_example_data</span><span class="p">,</span> <span class="n">plot_example</span>

<span class="c1"># Obtain example data.</span>
<span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">category_id</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">get_example_data</span><span class="p">()</span>

<span class="c1"># Linear approximation backprop.</span>
<span class="n">saliency</span> <span class="o">=</span> <span class="n">linear_approx</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">category_id</span><span class="p">,</span> <span class="n">saliency_layer</span><span class="o">=</span><span class="s1">&#39;features.29&#39;</span><span class="p">)</span>

<span class="c1"># Plots.</span>
<span class="n">plot_example</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">saliency</span><span class="p">,</span> <span class="s1">&#39;linear approx&#39;</span><span class="p">,</span> <span class="n">category_id</span><span class="p">)</span>
</pre></div>
</td></tr></table></div>
<p>Alternatively, it is possible to run the method “manually”. Linear
approximation is a variant of the gradient method, applied at an intermediate
layer:</p>
<div class="highlight-python notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchray.attribution.common</span> <span class="kn">import</span> <span class="n">Probe</span><span class="p">,</span> <span class="n">get_module</span>
<span class="kn">from</span> <span class="nn">torchray.attribution.linear_approx</span> <span class="kn">import</span> <span class="n">gradient_to_linear_approx_saliency</span>
<span class="kn">from</span> <span class="nn">torchray.benchmark</span> <span class="kn">import</span> <span class="n">get_example_data</span><span class="p">,</span> <span class="n">plot_example</span>

<span class="c1"># Obtain example data.</span>
<span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">category_id</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">get_example_data</span><span class="p">()</span>

<span class="c1"># Linear approximation.</span>
<span class="n">saliency_layer</span> <span class="o">=</span> <span class="n">get_module</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;features.29&#39;</span><span class="p">)</span>

<span class="n">probe</span> <span class="o">=</span> <span class="n">Probe</span><span class="p">(</span><span class="n">saliency_layer</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="s1">&#39;output&#39;</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">category_id</span><span class="p">]</span>
<span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="n">saliency</span> <span class="o">=</span> <span class="n">gradient_to_linear_approx_saliency</span><span class="p">(</span><span class="n">probe</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="c1"># Plots.</span>
<span class="n">plot_example</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">saliency</span><span class="p">,</span> <span class="s1">&#39;linear approx&#39;</span><span class="p">,</span> <span class="n">category_id</span><span class="p">)</span>
</pre></div>
</td></tr></table></div>
<p>Note that the function <a class="reference internal" href="#torchray.attribution.linear_approx.gradient_to_linear_approx_saliency" title="torchray.attribution.linear_approx.gradient_to_linear_approx_saliency"><code class="xref py py-func docutils literal notranslate"><span class="pre">gradient_to_linear_approx_saliency()</span></code></a> is used to
convert activations and gradients to a saliency map.</p>
<dl class="function">
<dt id="torchray.attribution.linear_approx.gradient_to_linear_approx_saliency">
<code class="sig-prename descclassname">torchray.attribution.linear_approx.</code><code class="sig-name descname">gradient_to_linear_approx_saliency</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchray/attribution/linear_approx.html#gradient_to_linear_approx_saliency"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchray.attribution.linear_approx.gradient_to_linear_approx_saliency" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the linear approximation of a tensor.</p>
<p>The tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">x</span></code> must have a valid gradient <code class="docutils literal notranslate"><span class="pre">x.grad</span></code>.
The function then computes the saliency map <span class="math notranslate nohighlight">\(s\)</span>: given by:</p>
<div class="math notranslate nohighlight">
\[s_{n1u} = \sum_{c} x_{ncu} \cdot dx_{ncu}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code>) – activation tensor with a valid gradient.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Saliency map.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torchray.attribution.linear_approx.linear_approx">
<code class="sig-prename descclassname">torchray.attribution.linear_approx.</code><code class="sig-name descname">linear_approx</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">gradient_to_saliency=&lt;function gradient_to_linear_approx_saliency&gt;</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchray/attribution/linear_approx.html#linear_approx"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchray.attribution.linear_approx.linear_approx" title="Permalink to this definition">¶</a></dt>
<dd><p>Linear approximation.</p>
<p>The function takes the same arguments as <a class="reference internal" href="#torchray.attribution.common.saliency" title="torchray.attribution.common.saliency"><code class="xref py py-func docutils literal notranslate"><span class="pre">common.saliency()</span></code></a>, with
the defaults required to apply the linear approximation method, and
supports the same arguments and return values.</p>
</dd></dl>

</div>
<div class="section" id="module-torchray.attribution.rise">
<span id="rise"></span><h2>RISE<a class="headerlink" href="#module-torchray.attribution.rise" title="Permalink to this headline">¶</a></h2>
<p>This module provides an implementation of the <em>RISE</em> method of <a class="reference internal" href="#id15" id="id14"><span>[RISE]</span></a> for
saliency visualization. This is given by the <a class="reference internal" href="#torchray.attribution.rise.rise" title="torchray.attribution.rise.rise"><code class="xref py py-func docutils literal notranslate"><span class="pre">rise()</span></code></a> function, which
can be used as follows:</p>
<div class="highlight-python notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchray.attribution.rise</span> <span class="kn">import</span> <span class="n">rise</span>
<span class="kn">from</span> <span class="nn">torchray.benchmark</span> <span class="kn">import</span> <span class="n">get_example_data</span><span class="p">,</span> <span class="n">plot_example</span>
<span class="kn">from</span> <span class="nn">torchray.utils</span> <span class="kn">import</span> <span class="n">get_device</span>

<span class="c1"># Obtain example data.</span>
<span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">category_id</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">get_example_data</span><span class="p">()</span>

<span class="c1"># Run on GPU if available.</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">get_device</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># RISE method.</span>
<span class="n">saliency</span> <span class="o">=</span> <span class="n">rise</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">saliency</span> <span class="o">=</span> <span class="n">saliency</span><span class="p">[:,</span> <span class="n">category_id</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Plots.</span>
<span class="n">plot_example</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">saliency</span><span class="p">,</span> <span class="s1">&#39;RISE&#39;</span><span class="p">,</span> <span class="n">category_id</span><span class="p">)</span>
</pre></div>
</td></tr></table></div>
<p class="rubric">References</p>
<dl class="citation">
<dt class="label" id="id15"><span class="brackets"><a class="fn-backref" href="#id14">RISE</a></span></dt>
<dd><p>V. Petsiuk, A. Das and K. Saenko
<em>RISE: Randomized Input Sampling for Explanation of Black-box
Models,</em>
BMVC 2018,
<a class="reference external" href="https://arxiv.org/pdf/1806.07421.pdf">https://arxiv.org/pdf/1806.07421.pdf</a>.</p>
</dd>
</dl>
<dl class="function">
<dt id="torchray.attribution.rise.rise">
<code class="sig-prename descclassname">torchray.attribution.rise.</code><code class="sig-name descname">rise</code><span class="sig-paren">(</span><em class="sig-param">model</em>, <em class="sig-param">input</em>, <em class="sig-param">target=None</em>, <em class="sig-param">seed=0</em>, <em class="sig-param">num_masks=8000</em>, <em class="sig-param">num_cells=7</em>, <em class="sig-param">filter_masks=None</em>, <em class="sig-param">batch_size=32</em>, <em class="sig-param">p=0.5</em>, <em class="sig-param">resize=False</em>, <em class="sig-param">resize_mode='bilinear'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchray/attribution/rise.html#rise"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchray.attribution.rise.rise" title="Permalink to this definition">¶</a></dt>
<dd><p>RISE.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>) – a model.</p></li>
<li><p><strong>input</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code>) – input tensor.</p></li>
<li><p><strong>seed</strong> (<em>int</em><em>, </em><em>optional</em>) – manual seed used to generate random numbers.
Default: <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p></li>
<li><p><strong>num_masks</strong> (<em>int</em><em>, </em><em>optional</em>) – number of RISE random masks to use.
Default: <code class="docutils literal notranslate"><span class="pre">8000</span></code>.</p></li>
<li><p><strong>num_cells</strong> (<em>int</em><em>, </em><em>optional</em>) – number of cells for one spatial dimension
in low-res RISE random mask. Default: <code class="docutils literal notranslate"><span class="pre">7</span></code>.</p></li>
<li><p><strong>filter_masks</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code>, optional) – If given, use the
provided pre-computed filter masks. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>batch_size</strong> (<em>int</em><em>, </em><em>optional</em>) – batch size to use. Default: <code class="docutils literal notranslate"><span class="pre">128</span></code>.</p></li>
<li><p><strong>p</strong> (<em>float</em><em>, </em><em>optional</em>) – with prob p, a low-res cell is set to 0;
otherwise, it’s 1. Default: <code class="docutils literal notranslate"><span class="pre">0.5</span></code>.</p></li>
<li><p><strong>resize</strong> (<em>bool</em><em> or </em><em>tuple of ints</em><em>, </em><em>optional</em>) – If True, resize saliency map
to size of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>. If False, don’t resize. If (width,
height) tuple, resize to (width, height). Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>resize_mode</strong> (<em>str</em><em>, </em><em>optional</em>) – If resize is not None, use this mode for
the resize function. Default: <code class="docutils literal notranslate"><span class="pre">'bilinear'</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>RISE saliency map.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torchray.attribution.rise.rise_class">
<code class="sig-prename descclassname">torchray.attribution.rise.</code><code class="sig-name descname">rise_class</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">target</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchray/attribution/rise.html#rise_class"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchray.attribution.rise.rise_class" title="Permalink to this definition">¶</a></dt>
<dd><p>Class-specific RISE.</p>
<p>This function has the all the arguments of <a class="reference internal" href="#torchray.attribution.rise.rise" title="torchray.attribution.rise.rise"><code class="xref py py-func docutils literal notranslate"><span class="pre">rise()</span></code></a> with the following
additional argument and returns a class-specific saliency map for the
given <code class="xref py py-attr docutils literal notranslate"><span class="pre">target</span></code> class(es).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>target</strong> (int, <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code>, list, or <code class="xref py py-class docutils literal notranslate"><span class="pre">np.ndarray</span></code>) – target label(s) that can be cast to <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.long</span></code>.</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-torchray.attribution.common">
<span id="common-code"></span><h2>Common code<a class="headerlink" href="#module-torchray.attribution.common" title="Permalink to this headline">¶</a></h2>
<p>This module defines common code for the backpropagation methods.</p>
<dl class="function">
<dt id="torchray.attribution.common.attach_debug_probes">
<code class="sig-prename descclassname">torchray.attribution.common.</code><code class="sig-name descname">attach_debug_probes</code><span class="sig-paren">(</span><em class="sig-param">model</em>, <em class="sig-param">debug=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchray/attribution/common.html#attach_debug_probes"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchray.attribution.common.attach_debug_probes" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an <code class="xref py py-class docutils literal notranslate"><span class="pre">collections.OrderedDict</span></code> of <a class="reference internal" href="#torchray.attribution.common.Probe" title="torchray.attribution.common.Probe"><code class="xref py py-class docutils literal notranslate"><span class="pre">Probe</span></code></a> objects for
all modules in the model if <code class="xref py py-attr docutils literal notranslate"><span class="pre">debug</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>; otherwise, returns
<code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>) – a model.</p></li>
<li><p><strong>debug</strong> (<em>bool</em><em>, </em><em>optional</em>) – if True, return an OrderedDict of Probe objects
for all modules in the model; otherwise returns <code class="docutils literal notranslate"><span class="pre">None</span></code>.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>dict of <a class="reference internal" href="#torchray.attribution.common.Probe" title="torchray.attribution.common.Probe"><code class="xref py py-class docutils literal notranslate"><span class="pre">Probe</span></code></a> objects for</dt><dd><p>all modules in the model.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">collections.OrderedDict</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torchray.attribution.common.get_backward_gradient">
<code class="sig-prename descclassname">torchray.attribution.common.</code><code class="sig-name descname">get_backward_gradient</code><span class="sig-paren">(</span><em class="sig-param">pred_y</em>, <em class="sig-param">y</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchray/attribution/common.html#get_backward_gradient"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchray.attribution.common.get_backward_gradient" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a gradient tensor that is either equal to <code class="xref py py-attr docutils literal notranslate"><span class="pre">y</span></code> (if y is a
tensor with the same shape as pred_y) or a one-hot encoding in the channels
dimension.</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">y</span></code> can be either an <code class="docutils literal notranslate"><span class="pre">int</span></code>, an array-like list of integers,
or a tensor. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">y</span></code> is a tensor with the same shape as
<code class="xref py py-attr docutils literal notranslate"><span class="pre">pred_y</span></code>, the function returns <code class="xref py py-attr docutils literal notranslate"><span class="pre">y</span></code> unchanged.</p>
<p>Otherwise, <code class="xref py py-attr docutils literal notranslate"><span class="pre">y</span></code> is interpreted as a list of class indices. These
are first unfolded/expanded to one index per batch element in
<code class="xref py py-attr docutils literal notranslate"><span class="pre">pred_y</span></code> (i.e. along the first dimension). Then, this list
is further expanded to all spatial dimensions of <code class="xref py py-attr docutils literal notranslate"><span class="pre">pred_y</span></code>.
(i.e. all but the first two dimensions of <code class="xref py py-attr docutils literal notranslate"><span class="pre">pred_y</span></code>).
Finally, the function return a “gradient” tensor that is a one-hot
indicator tensor for these classes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pred_y</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code>) – model output tensor.</p></li>
<li><p><strong>y</strong> (int, <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code>, list, or <code class="xref py py-class docutils literal notranslate"><span class="pre">np.ndarray</span></code>) – target
label(s) that can be cast to <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.long</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>gradient tensor with the same shape as</dt><dd><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">pred_y</span></code>.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torchray.attribution.common.get_module">
<code class="sig-prename descclassname">torchray.attribution.common.</code><code class="sig-name descname">get_module</code><span class="sig-paren">(</span><em class="sig-param">model</em>, <em class="sig-param">module</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchray/attribution/common.html#get_module"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchray.attribution.common.get_module" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a specific layer in a model based.</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">module</span></code> is either the name of a module (as given by the
<code class="xref py py-func docutils literal notranslate"><span class="pre">named_modules()</span></code> function for <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> objects) or
a <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> object. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">module</span></code> is a
<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> object, then <code class="xref py py-attr docutils literal notranslate"><span class="pre">module</span></code> is returned unchanged.
If <code class="xref py py-attr docutils literal notranslate"><span class="pre">module</span></code> is a str, the function searches for a module with the
name <code class="xref py py-attr docutils literal notranslate"><span class="pre">module</span></code> and returns a <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> if found;
otherwise, <code class="docutils literal notranslate"><span class="pre">None</span></code> is returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>) – model in which to search for layer.</p></li>
<li><p><strong>module</strong> (str or <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>) – name of layer (str) or the
layer itself (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>specific PyTorch layer (<code class="docutils literal notranslate"><span class="pre">None</span></code> if the layer</dt><dd><p>isn’t found).</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torchray.attribution.common.get_pointing_gradient">
<code class="sig-prename descclassname">torchray.attribution.common.</code><code class="sig-name descname">get_pointing_gradient</code><span class="sig-paren">(</span><em class="sig-param">pred_y</em>, <em class="sig-param">y</em>, <em class="sig-param">normalize=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchray/attribution/common.html#get_pointing_gradient"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchray.attribution.common.get_pointing_gradient" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a gradient tensor for the pointing game.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pred_y</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code>) – 4D tensor that the model outputs.</p></li>
<li><p><strong>y</strong> (<em>int</em>) – target label.</p></li>
<li><p><strong>normalize</strong> (<em>bool</em>) – If True, normalize the gradient tensor s.t. it
sums to 1. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>gradient tensor with the same shape as
<code class="xref py py-attr docutils literal notranslate"><span class="pre">pred_y</span></code>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torchray.attribution.common.gradient_to_saliency">
<code class="sig-prename descclassname">torchray.attribution.common.</code><code class="sig-name descname">gradient_to_saliency</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchray/attribution/common.html#gradient_to_saliency"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchray.attribution.common.gradient_to_saliency" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert a gradient to a saliency map.</p>
<p>The tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">x</span></code> must have a valid gradient <code class="docutils literal notranslate"><span class="pre">x.grad</span></code>.
The function then computes the saliency map <span class="math notranslate nohighlight">\(s\)</span> given by:</p>
<div class="math notranslate nohighlight">
\[s_{n,1,u} = \max_{0 \leq c &lt; C} |dx_{ncu}|\]</div>
<p>where <span class="math notranslate nohighlight">\(n\)</span> is the instance index, <span class="math notranslate nohighlight">\(c\)</span> the channel
index and <span class="math notranslate nohighlight">\(u\)</span> the spatial multi-index (usually of dimension 2 for
images).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>Tensor</em>) – activation with gradient.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>saliency</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torchray.attribution.common.Probe">
<em class="property">class </em><code class="sig-prename descclassname">torchray.attribution.common.</code><code class="sig-name descname">Probe</code><span class="sig-paren">(</span><em class="sig-param">module</em>, <em class="sig-param">target='input'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchray/attribution/common.html#Probe"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchray.attribution.common.Probe" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Probe for a layer.</p>
<p>A probe attaches to a given <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> instance.
While attached, the object records any data produced by the module along
with the corresponding gradients. Use <a class="reference internal" href="#torchray.attribution.common.Probe.remove" title="torchray.attribution.common.Probe.remove"><code class="xref py py-func docutils literal notranslate"><span class="pre">remove()</span></code></a> to remove the probe.</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">module</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span>
<span class="n">probe</span> <span class="o">=</span> <span class="n">Probe</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">probe</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">probe</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<dl class="method">
<dt id="torchray.attribution.common.Probe.remove">
<code class="sig-name descname">remove</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchray/attribution/common.html#Probe.remove"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchray.attribution.common.Probe.remove" title="Permalink to this definition">¶</a></dt>
<dd><p>Remove the probe.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchray.attribution.common.Patch">
<em class="property">class </em><code class="sig-prename descclassname">torchray.attribution.common.</code><code class="sig-name descname">Patch</code><span class="sig-paren">(</span><em class="sig-param">target</em>, <em class="sig-param">new_callable</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchray/attribution/common.html#Patch"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchray.attribution.common.Patch" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Patch a callable in a module.</p>
<dl class="method">
<dt id="torchray.attribution.common.Patch.remove">
<code class="sig-name descname">remove</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchray/attribution/common.html#Patch.remove"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchray.attribution.common.Patch.remove" title="Permalink to this definition">¶</a></dt>
<dd><p>Remove the patch.</p>
</dd></dl>

<dl class="method">
<dt id="torchray.attribution.common.Patch.resolve">
<em class="property">static </em><code class="sig-name descname">resolve</code><span class="sig-paren">(</span><em class="sig-param">target</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchray/attribution/common.html#Patch.resolve"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchray.attribution.common.Patch.resolve" title="Permalink to this definition">¶</a></dt>
<dd><p>Resolve a target into a module and an attribute.</p>
<p>The function resolves a string such as <code class="docutils literal notranslate"><span class="pre">'this.that.thing'</span></code> into a
module instance <cite>this.that</cite> (importing the module) and an attribute
<cite>thing</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>target</strong> (<em>str</em>) – target string.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>module, attribute.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tuple</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchray.attribution.common.ReLUContext">
<em class="property">class </em><code class="sig-prename descclassname">torchray.attribution.common.</code><code class="sig-name descname">ReLUContext</code><span class="sig-paren">(</span><em class="sig-param">relu_func</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchray/attribution/common.html#ReLUContext"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchray.attribution.common.ReLUContext" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="simple">
<dt>A context manager that replaces <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.relu()</span></code> with</dt><dd><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">relu_function</span></code>.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>relu_func</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.function.FunctionMeta</span></code>) – class
definition of a <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torchray.attribution.common.resize_saliency">
<code class="sig-prename descclassname">torchray.attribution.common.</code><code class="sig-name descname">resize_saliency</code><span class="sig-paren">(</span><em class="sig-param">tensor</em>, <em class="sig-param">saliency</em>, <em class="sig-param">size</em>, <em class="sig-param">mode</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchray/attribution/common.html#resize_saliency"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchray.attribution.common.resize_saliency" title="Permalink to this definition">¶</a></dt>
<dd><p>Resize a saliency map.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code>) – reference tensor.</p></li>
<li><p><strong>saliency</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code>) – saliency map.</p></li>
<li><p><strong>size</strong> (<em>bool</em><em> or </em><em>tuple of int</em>) – if a tuple (i.e., (width, height),
resize <a class="reference internal" href="#torchray.attribution.common.saliency" title="torchray.attribution.common.saliency"><code class="xref py py-attr docutils literal notranslate"><span class="pre">saliency</span></code></a> to <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code>. If True, resize
<code class="xref py py-attr docutils literal notranslate"><span class="pre">saliency:</span> <span class="pre">to</span> <span class="pre">the</span> <span class="pre">shape</span> <span class="pre">of</span> <span class="pre">:attr:`tensor</span></code>; otherwise,
return <a class="reference internal" href="#torchray.attribution.common.saliency" title="torchray.attribution.common.saliency"><code class="xref py py-attr docutils literal notranslate"><span class="pre">saliency</span></code></a> unchanged.</p></li>
<li><p><strong>mode</strong> (<em>str</em>) – mode for <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.interpolate()</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Resized saliency map.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torchray.attribution.common.saliency">
<code class="sig-prename descclassname">torchray.attribution.common.</code><code class="sig-name descname">saliency</code><span class="sig-paren">(</span><em class="sig-param">model</em>, <em class="sig-param">input</em>, <em class="sig-param">target</em>, <em class="sig-param">saliency_layer=''</em>, <em class="sig-param">resize=False</em>, <em class="sig-param">resize_mode='bilinear'</em>, <em class="sig-param">smooth=0</em>, <em class="sig-param">context_builder=&lt;class 'torchray.attribution.common.NullContext'&gt;</em>, <em class="sig-param">gradient_to_saliency=&lt;function gradient_to_saliency&gt;</em>, <em class="sig-param">get_backward_gradient=&lt;function get_backward_gradient&gt;</em>, <em class="sig-param">debug=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchray/attribution/common.html#saliency"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchray.attribution.common.saliency" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply a backprop-based attribution method to an image.</p>
<p>The saliency method is specified by a suitable context factory
<code class="xref py py-attr docutils literal notranslate"><span class="pre">context_builder</span></code>. This context is used to modify the backpropagation
algorithm to match a given visualization method. This:</p>
<ol class="arabic simple">
<li><p>Attaches a probe to the output tensor of <code class="xref py py-attr docutils literal notranslate"><span class="pre">saliency_layer</span></code>,
which must be a layer in <code class="xref py py-attr docutils literal notranslate"><span class="pre">model</span></code>. If no such layer is specified,
it selects the input tensor to <code class="xref py py-attr docutils literal notranslate"><span class="pre">model</span></code>.</p></li>
<li><p>Uses the function <a class="reference internal" href="#torchray.attribution.common.get_backward_gradient" title="torchray.attribution.common.get_backward_gradient"><code class="xref py py-attr docutils literal notranslate"><span class="pre">get_backward_gradient</span></code></a> to obtain a gradient
for the output tensor of the model. This function is passed
as input the output tensor as well as the parameter <code class="xref py py-attr docutils literal notranslate"><span class="pre">target</span></code>.
By default, the <a class="reference internal" href="#torchray.attribution.common.get_backward_gradient" title="torchray.attribution.common.get_backward_gradient"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_backward_gradient()</span></code></a> function is used.
The latter generates as gradient a one-hot vector selecting
<code class="xref py py-attr docutils literal notranslate"><span class="pre">target</span></code>, usually the index of the class predicted by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">model</span></code>.</p></li>
<li><p>Evaluates <code class="xref py py-attr docutils literal notranslate"><span class="pre">model</span></code> on <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and then computes the
pseudo-gradient of the model with respect the selected tensor. This
calculation is controlled by <code class="xref py py-attr docutils literal notranslate"><span class="pre">context_builder</span></code>.</p></li>
<li><p>Extract the pseudo-gradient at the selected tensor as a raw saliency
map.</p></li>
<li><p>Call <a class="reference internal" href="#torchray.attribution.common.gradient_to_saliency" title="torchray.attribution.common.gradient_to_saliency"><code class="xref py py-attr docutils literal notranslate"><span class="pre">gradient_to_saliency</span></code></a> to obtain an actual saliency map.
This defaults to <a class="reference internal" href="#torchray.attribution.common.gradient_to_saliency" title="torchray.attribution.common.gradient_to_saliency"><code class="xref py py-func docutils literal notranslate"><span class="pre">gradient_to_saliency()</span></code></a> that takes the maximum
absolute value along the channel dimension of the pseudo-gradient
tensor.</p></li>
<li><p>Optionally resizes the saliency map thus obtained. By default,
this uses bilinear interpolation and resizes the saliency to the same
spatial dimension of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></li>
<li><p>Optionally applies a Gaussian filter to the resized saliency map.
The standard deviation <code class="xref py py-attr docutils literal notranslate"><span class="pre">sigma</span></code> of this filter is measured
as a fraction of the maxmum spatial dimension of the resized
saliency map.</p></li>
<li><p>Removes the probe.</p></li>
<li><p>Returns the saliency map or optionally a tuple with the saliency map
and a OrderedDict of Probe objects for all modules in the model, which
can be used for debugging.</p></li>
</ol>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>) – a model.</p></li>
<li><p><strong>input</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code>) – input tensor.</p></li>
<li><p><strong>target</strong> (int or <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code>) – target label(s).</p></li>
<li><p><strong>saliency_layer</strong> (str or <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>, optional) – name of the
saliency layer (str) or the layer itself (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>)
in the model at which to visualize. Default: <code class="docutils literal notranslate"><span class="pre">''</span></code> (visualize
at input).</p></li>
<li><p><strong>resize</strong> (<em>bool</em><em> or </em><em>tuple</em><em>, </em><em>optional</em>) – if True, upsample saliency map to the
same size as <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>. It is also possible to specify a pair
(width, height) for a different size. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>resize_mode</strong> (<em>str</em><em>, </em><em>optional</em>) – upsampling method to use. Default:
<code class="docutils literal notranslate"><span class="pre">'bilinear'</span></code>.</p></li>
<li><p><strong>smooth</strong> (<em>float</em><em>, </em><em>optional</em>) – amount of Gaussian smoothing to apply to the
saliency map. Default: <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p></li>
<li><p><strong>context_builder</strong> (<em>type</em><em>, </em><em>optional</em>) – type of context to use. Default:
<code class="xref py py-class docutils literal notranslate"><span class="pre">NullContext</span></code>.</p></li>
<li><p><strong>gradient_to_saliency</strong> (<em>function</em><em>, </em><em>optional</em>) – function that converts the
pseudo-gradient signal to a saliency map. Default:
<a class="reference internal" href="#torchray.attribution.common.gradient_to_saliency" title="torchray.attribution.common.gradient_to_saliency"><code class="xref py py-func docutils literal notranslate"><span class="pre">gradient_to_saliency()</span></code></a>.</p></li>
<li><p><strong>get_backward_gradient</strong> (<em>function</em><em>, </em><em>optional</em>) – function that generates
gradient tensor to backpropagate. Default:
<a class="reference internal" href="#torchray.attribution.common.get_backward_gradient" title="torchray.attribution.common.get_backward_gradient"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_backward_gradient()</span></code></a>.</p></li>
<li><p><strong>debug</strong> (<em>bool</em><em>, </em><em>optional</em>) – if True, also return an
<code class="xref py py-class docutils literal notranslate"><span class="pre">collections.OrderedDict</span></code> of <a class="reference internal" href="#torchray.attribution.common.Probe" title="torchray.attribution.common.Probe"><code class="xref py py-class docutils literal notranslate"><span class="pre">Probe</span></code></a> objects for
all modules in the model. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">debug</span></code> is False, returns a
<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code> saliency map at <code class="xref py py-attr docutils literal notranslate"><span class="pre">saliency_layer</span></code>.
Otherwise, returns a tuple of a <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code> saliency map
at <code class="xref py py-attr docutils literal notranslate"><span class="pre">saliency_layer</span></code> and an <code class="xref py py-class docutils literal notranslate"><span class="pre">collections.OrderedDict</span></code>
of <a class="reference internal" href="#torchray.attribution.common.Probe" title="torchray.attribution.common.Probe"><code class="xref py py-class docutils literal notranslate"><span class="pre">Probe</span></code></a> objects for all modules in the model.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code> or tuple</p>
</dd>
</dl>
</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="benchmark.html" class="btn btn-neutral float-right" title="Benchmarking" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="index.html" class="btn btn-neutral float-left" title="Welcome to TorchRay" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright TorchRay Contributors

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>